{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "746c3d7a-f4e4-4505-b58c-2c93ea8c3cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string, re, nltk, os\n",
    "from nltk.corpus import stopwords\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fe117c26-15a3-4f0c-bd3f-1579fa9b8cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the doc in memory\n",
    "def load_doc(filename):\n",
    "    f = open(filename)\n",
    "    text = f.read()\n",
    "    f.close()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8f7c0b35-7fb9-4eb4-9504-0b3a6c9d9c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "swords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3b2f2d88-50a2-4171-b66f-f229f0d19b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean the doc\n",
    "def clean_doc(doc):\n",
    "    tokens = word_tokenize(doc)\n",
    "    tokens = [token for token in tokens if token.isalpha()]\n",
    "    tokens = [token for token in tokens if token not in swords]\n",
    "    tokens = [token for token in tokens if len(token) > 1]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b3ade247-99fe-4715-81ad-14d356a72cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data = load_doc('datasets/review_polarity/txt_sentoken/pos/cv000_29590.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d09321b1-3f59-4637-af2d-92d8f0f5e30a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['films',\n",
       " 'adapted',\n",
       " 'comic',\n",
       " 'books',\n",
       " 'plenty',\n",
       " 'success',\n",
       " 'whether',\n",
       " 'superheroes',\n",
       " 'batman',\n",
       " 'superman',\n",
       " 'spawn',\n",
       " 'geared',\n",
       " 'toward',\n",
       " 'kids',\n",
       " 'casper',\n",
       " 'arthouse',\n",
       " 'crowd',\n",
       " 'ghost',\n",
       " 'world',\n",
       " 'never',\n",
       " 'really',\n",
       " 'comic',\n",
       " 'book',\n",
       " 'like',\n",
       " 'hell',\n",
       " 'starters',\n",
       " 'created',\n",
       " 'alan',\n",
       " 'moore',\n",
       " 'eddie',\n",
       " 'campbell',\n",
       " 'brought',\n",
       " 'medium',\n",
       " 'whole',\n",
       " 'new',\n",
       " 'level',\n",
       " 'mid',\n",
       " 'series',\n",
       " 'called',\n",
       " 'watchmen',\n",
       " 'say',\n",
       " 'moore',\n",
       " 'campbell',\n",
       " 'thoroughly',\n",
       " 'researched',\n",
       " 'subject',\n",
       " 'jack',\n",
       " 'ripper',\n",
       " 'would',\n",
       " 'like',\n",
       " 'saying',\n",
       " 'michael',\n",
       " 'jackson',\n",
       " 'starting',\n",
       " 'look',\n",
       " 'little',\n",
       " 'odd',\n",
       " 'book',\n",
       " 'graphic',\n",
       " 'novel',\n",
       " 'pages',\n",
       " 'long',\n",
       " 'includes',\n",
       " 'nearly',\n",
       " 'consist',\n",
       " 'nothing',\n",
       " 'footnotes',\n",
       " 'words',\n",
       " 'dismiss',\n",
       " 'film',\n",
       " 'source',\n",
       " 'get',\n",
       " 'past',\n",
       " 'whole',\n",
       " 'comic',\n",
       " 'book',\n",
       " 'thing',\n",
       " 'might',\n",
       " 'find',\n",
       " 'another',\n",
       " 'stumbling',\n",
       " 'block',\n",
       " 'hell',\n",
       " 'directors',\n",
       " 'albert',\n",
       " 'allen',\n",
       " 'hughes',\n",
       " 'getting',\n",
       " 'hughes',\n",
       " 'brothers',\n",
       " 'direct',\n",
       " 'seems',\n",
       " 'almost',\n",
       " 'ludicrous',\n",
       " 'casting',\n",
       " 'carrot',\n",
       " 'top',\n",
       " 'well',\n",
       " 'anything',\n",
       " 'riddle',\n",
       " 'better',\n",
       " 'direct',\n",
       " 'film',\n",
       " 'set',\n",
       " 'ghetto',\n",
       " 'features',\n",
       " 'really',\n",
       " 'violent',\n",
       " 'street',\n",
       " 'crime',\n",
       " 'mad',\n",
       " 'geniuses',\n",
       " 'behind',\n",
       " 'menace',\n",
       " 'ii',\n",
       " 'society',\n",
       " 'ghetto',\n",
       " 'question',\n",
       " 'course',\n",
       " 'whitechapel',\n",
       " 'london',\n",
       " 'east',\n",
       " 'end',\n",
       " 'filthy',\n",
       " 'sooty',\n",
       " 'place',\n",
       " 'whores',\n",
       " 'called',\n",
       " 'unfortunates',\n",
       " 'starting',\n",
       " 'get',\n",
       " 'little',\n",
       " 'nervous',\n",
       " 'mysterious',\n",
       " 'psychopath',\n",
       " 'carving',\n",
       " 'profession',\n",
       " 'surgical',\n",
       " 'precision',\n",
       " 'first',\n",
       " 'stiff',\n",
       " 'turns',\n",
       " 'copper',\n",
       " 'peter',\n",
       " 'godley',\n",
       " 'robbie',\n",
       " 'coltrane',\n",
       " 'world',\n",
       " 'enough',\n",
       " 'calls',\n",
       " 'inspector',\n",
       " 'frederick',\n",
       " 'abberline',\n",
       " 'johnny',\n",
       " 'depp',\n",
       " 'blow',\n",
       " 'crack',\n",
       " 'case',\n",
       " 'abberline',\n",
       " 'widower',\n",
       " 'prophetic',\n",
       " 'dreams',\n",
       " 'unsuccessfully',\n",
       " 'tries',\n",
       " 'quell',\n",
       " 'copious',\n",
       " 'amounts',\n",
       " 'absinthe',\n",
       " 'opium',\n",
       " 'upon',\n",
       " 'arriving',\n",
       " 'whitechapel',\n",
       " 'befriends',\n",
       " 'unfortunate',\n",
       " 'named',\n",
       " 'mary',\n",
       " 'kelly',\n",
       " 'heather',\n",
       " 'graham',\n",
       " 'say',\n",
       " 'proceeds',\n",
       " 'investigate',\n",
       " 'horribly',\n",
       " 'gruesome',\n",
       " 'crimes',\n",
       " 'even',\n",
       " 'police',\n",
       " 'surgeon',\n",
       " 'ca',\n",
       " 'stomach',\n",
       " 'think',\n",
       " 'anyone',\n",
       " 'needs',\n",
       " 'briefed',\n",
       " 'jack',\n",
       " 'ripper',\n",
       " 'wo',\n",
       " 'go',\n",
       " 'particulars',\n",
       " 'say',\n",
       " 'moore',\n",
       " 'campbell',\n",
       " 'unique',\n",
       " 'interesting',\n",
       " 'theory',\n",
       " 'identity',\n",
       " 'killer',\n",
       " 'reasons',\n",
       " 'chooses',\n",
       " 'slay',\n",
       " 'comic',\n",
       " 'bother',\n",
       " 'cloaking',\n",
       " 'identity',\n",
       " 'ripper',\n",
       " 'screenwriters',\n",
       " 'terry',\n",
       " 'hayes',\n",
       " 'vertical',\n",
       " 'limit',\n",
       " 'rafael',\n",
       " 'yglesias',\n",
       " 'les',\n",
       " 'mis',\n",
       " 'rables',\n",
       " 'good',\n",
       " 'job',\n",
       " 'keeping',\n",
       " 'hidden',\n",
       " 'viewers',\n",
       " 'end',\n",
       " 'funny',\n",
       " 'watch',\n",
       " 'locals',\n",
       " 'blindly',\n",
       " 'point',\n",
       " 'finger',\n",
       " 'blame',\n",
       " 'jews',\n",
       " 'indians',\n",
       " 'englishman',\n",
       " 'could',\n",
       " 'never',\n",
       " 'capable',\n",
       " 'committing',\n",
       " 'ghastly',\n",
       " 'acts',\n",
       " 'hell',\n",
       " 'ending',\n",
       " 'whistling',\n",
       " 'stonecutters',\n",
       " 'song',\n",
       " 'simpsons',\n",
       " 'days',\n",
       " 'holds',\n",
       " 'back',\n",
       " 'electric',\n",
       " 'made',\n",
       " 'steve',\n",
       " 'guttenberg',\n",
       " 'star',\n",
       " 'worry',\n",
       " 'make',\n",
       " 'sense',\n",
       " 'see',\n",
       " 'onto',\n",
       " 'hell',\n",
       " 'appearance',\n",
       " 'certainly',\n",
       " 'dark',\n",
       " 'bleak',\n",
       " 'enough',\n",
       " 'surprising',\n",
       " 'see',\n",
       " 'much',\n",
       " 'looks',\n",
       " 'like',\n",
       " 'tim',\n",
       " 'burton',\n",
       " 'film',\n",
       " 'planet',\n",
       " 'apes',\n",
       " 'times',\n",
       " 'seems',\n",
       " 'like',\n",
       " 'sleepy',\n",
       " 'hollow',\n",
       " 'print',\n",
       " 'saw',\n",
       " 'completely',\n",
       " 'finished',\n",
       " 'color',\n",
       " 'music',\n",
       " 'finalized',\n",
       " 'comments',\n",
       " 'marilyn',\n",
       " 'manson',\n",
       " 'cinematographer',\n",
       " 'peter',\n",
       " 'deming',\n",
       " 'say',\n",
       " 'word',\n",
       " 'ably',\n",
       " 'captures',\n",
       " 'dreariness',\n",
       " 'london',\n",
       " 'helped',\n",
       " 'make',\n",
       " 'flashy',\n",
       " 'killing',\n",
       " 'scenes',\n",
       " 'remind',\n",
       " 'crazy',\n",
       " 'flashbacks',\n",
       " 'twin',\n",
       " 'peaks',\n",
       " 'even',\n",
       " 'though',\n",
       " 'violence',\n",
       " 'film',\n",
       " 'pales',\n",
       " 'comparison',\n",
       " 'comic',\n",
       " 'oscar',\n",
       " 'winner',\n",
       " 'martin',\n",
       " 'childs',\n",
       " 'shakespeare',\n",
       " 'love',\n",
       " 'production',\n",
       " 'design',\n",
       " 'turns',\n",
       " 'original',\n",
       " 'prague',\n",
       " 'surroundings',\n",
       " 'one',\n",
       " 'creepy',\n",
       " 'place',\n",
       " 'even',\n",
       " 'acting',\n",
       " 'hell',\n",
       " 'solid',\n",
       " 'dreamy',\n",
       " 'depp',\n",
       " 'turning',\n",
       " 'typically',\n",
       " 'strong',\n",
       " 'performance',\n",
       " 'deftly',\n",
       " 'handling',\n",
       " 'british',\n",
       " 'accent',\n",
       " 'ians',\n",
       " 'holm',\n",
       " 'joe',\n",
       " 'gould',\n",
       " 'secret',\n",
       " 'richardson',\n",
       " 'dalmatians',\n",
       " 'log',\n",
       " 'great',\n",
       " 'supporting',\n",
       " 'roles',\n",
       " 'big',\n",
       " 'surprise',\n",
       " 'graham',\n",
       " 'cringed',\n",
       " 'first',\n",
       " 'time',\n",
       " 'opened',\n",
       " 'mouth',\n",
       " 'imagining',\n",
       " 'attempt',\n",
       " 'irish',\n",
       " 'accent',\n",
       " 'actually',\n",
       " 'half',\n",
       " 'bad',\n",
       " 'film',\n",
       " 'however',\n",
       " 'good',\n",
       " 'strong',\n",
       " 'sexuality',\n",
       " 'language',\n",
       " 'drug',\n",
       " 'content']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_doc(sample_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9a3a4a07-4c20-4bed-bf48-e47dee34c6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the file, clean the data and return lines\n",
    "def doc_to_line(filename):\n",
    "    doc = load_doc(filename)\n",
    "    tokens = clean_doc(doc)\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0a7ad40c-249b-4298-b635-c7b588d26bc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'films adapted comic books plenty success whether superheroes batman superman spawn geared toward kids casper arthouse crowd ghost world never really comic book like hell starters created alan moore eddie campbell brought medium whole new level mid series called watchmen say moore campbell thoroughly researched subject jack ripper would like saying michael jackson starting look little odd book graphic novel pages long includes nearly consist nothing footnotes words dismiss film source get past whole comic book thing might find another stumbling block hell directors albert allen hughes getting hughes brothers direct seems almost ludicrous casting carrot top well anything riddle better direct film set ghetto features really violent street crime mad geniuses behind menace ii society ghetto question course whitechapel london east end filthy sooty place whores called unfortunates starting get little nervous mysterious psychopath carving profession surgical precision first stiff turns copper peter godley robbie coltrane world enough calls inspector frederick abberline johnny depp blow crack case abberline widower prophetic dreams unsuccessfully tries quell copious amounts absinthe opium upon arriving whitechapel befriends unfortunate named mary kelly heather graham say proceeds investigate horribly gruesome crimes even police surgeon ca stomach think anyone needs briefed jack ripper wo go particulars say moore campbell unique interesting theory identity killer reasons chooses slay comic bother cloaking identity ripper screenwriters terry hayes vertical limit rafael yglesias les mis rables good job keeping hidden viewers end funny watch locals blindly point finger blame jews indians englishman could never capable committing ghastly acts hell ending whistling stonecutters song simpsons days holds back electric made steve guttenberg star worry make sense see onto hell appearance certainly dark bleak enough surprising see much looks like tim burton film planet apes times seems like sleepy hollow print saw completely finished color music finalized comments marilyn manson cinematographer peter deming say word ably captures dreariness london helped make flashy killing scenes remind crazy flashbacks twin peaks even though violence film pales comparison comic oscar winner martin childs shakespeare love production design turns original prague surroundings one creepy place even acting hell solid dreamy depp turning typically strong performance deftly handling british accent ians holm joe gould secret richardson dalmatians log great supporting roles big surprise graham cringed first time opened mouth imagining attempt irish accent actually half bad film however good strong sexuality language drug content'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_to_line('datasets/review_polarity/txt_sentoken/pos/cv000_29590.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2c96a960-6da5-4818-b38f-49d93c5f26ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sometimes',\n",
       " 'tip',\n",
       " 'hat',\n",
       " 'film',\n",
       " 'jump',\n",
       " 'bandwagon',\n",
       " 'enjoy',\n",
       " 'ride',\n",
       " 'saw',\n",
       " 'truman',\n",
       " 'show',\n",
       " 'audience',\n",
       " 'full',\n",
       " 'teenagers',\n",
       " 'doubt',\n",
       " 'drawn',\n",
       " 'ace',\n",
       " 'ventura',\n",
       " 'hoping',\n",
       " 'see',\n",
       " 'latest',\n",
       " 'take',\n",
       " 'fart',\n",
       " 'jokes',\n",
       " 'surprised',\n",
       " 'may',\n",
       " 'realized',\n",
       " 'picture',\n",
       " 'actually',\n",
       " 'something',\n",
       " 'say',\n",
       " 'crowd',\n",
       " 'id',\n",
       " 'cant',\n",
       " 'tell',\n",
       " 'yet',\n",
       " 'really',\n",
       " 'good',\n",
       " 'seems',\n",
       " 'comparison',\n",
       " 'awful',\n",
       " 'mountain',\n",
       " 'crap',\n",
       " 'spewed',\n",
       " 'forth',\n",
       " 'bowls',\n",
       " 'hollywood',\n",
       " 'far',\n",
       " 'year',\n",
       " 'time',\n",
       " 'alone',\n",
       " 'make',\n",
       " 'call',\n",
       " 'end',\n",
       " 'burbanks',\n",
       " 'side',\n",
       " 'concerned',\n",
       " 'wanted',\n",
       " 'win',\n",
       " 'felt',\n",
       " 'tribute',\n",
       " 'jim',\n",
       " 'carrey',\n",
       " 'achieved',\n",
       " 'legitimacy',\n",
       " 'last',\n",
       " 'best',\n",
       " 'viewed',\n",
       " 'cold',\n",
       " 'little',\n",
       " 'possible',\n",
       " 'plot',\n",
       " 'unless',\n",
       " 'youre',\n",
       " 'media',\n",
       " 'blackout',\n",
       " 'however',\n",
       " 'probably',\n",
       " 'know',\n",
       " 'basics',\n",
       " 'stars',\n",
       " 'burbank',\n",
       " 'everyman',\n",
       " 'insurance',\n",
       " 'agent',\n",
       " 'living',\n",
       " 'beautiful',\n",
       " 'wife',\n",
       " 'meryl',\n",
       " 'laura',\n",
       " 'linney',\n",
       " 'south',\n",
       " 'florida',\n",
       " 'island',\n",
       " 'town',\n",
       " 'seahaven',\n",
       " 'looks',\n",
       " 'like',\n",
       " 'michael',\n",
       " 'idea',\n",
       " 'perfect',\n",
       " 'american',\n",
       " 'small',\n",
       " 'would',\n",
       " 'seem',\n",
       " 'live',\n",
       " 'middle',\n",
       " 'class',\n",
       " 'lifestyle',\n",
       " 'complete',\n",
       " 'working',\n",
       " 'bud',\n",
       " 'friendly',\n",
       " 'neighbors',\n",
       " 'interesting',\n",
       " 'coworkers',\n",
       " 'tragedy',\n",
       " 'past',\n",
       " 'father',\n",
       " 'drowned',\n",
       " 'horrible',\n",
       " 'boating',\n",
       " 'accident',\n",
       " 'leaving',\n",
       " 'dreadful',\n",
       " 'fear',\n",
       " 'water',\n",
       " 'travel',\n",
       " 'general',\n",
       " 'overall',\n",
       " 'life',\n",
       " 'one',\n",
       " 'day',\n",
       " 'leaves',\n",
       " 'house',\n",
       " 'work',\n",
       " 'light',\n",
       " 'falls',\n",
       " 'magically',\n",
       " 'sky',\n",
       " 'curious',\n",
       " 'event',\n",
       " 'lead',\n",
       " 'discover',\n",
       " 'rest',\n",
       " 'world',\n",
       " 'already',\n",
       " 'knows',\n",
       " 'prisoner',\n",
       " 'worlds',\n",
       " 'biggest',\n",
       " 'soundstage',\n",
       " 'friends',\n",
       " 'relatives',\n",
       " 'actors',\n",
       " 'paid',\n",
       " 'interact',\n",
       " 'days',\n",
       " 'broadcast',\n",
       " 'popular',\n",
       " 'television',\n",
       " 'program',\n",
       " 'makes',\n",
       " 'first',\n",
       " 'hour',\n",
       " 'enjoyable',\n",
       " 'watch',\n",
       " 'magical',\n",
       " 'verisimilitude',\n",
       " 'painstakingly',\n",
       " 'constructed',\n",
       " 'niccol',\n",
       " 'weir',\n",
       " 'told',\n",
       " 'second',\n",
       " 'manmade',\n",
       " 'structure',\n",
       " 'visible',\n",
       " 'space',\n",
       " 'trumans',\n",
       " 'movements',\n",
       " 'tracked',\n",
       " 'ceaselessly',\n",
       " 'cameras',\n",
       " 'scattered',\n",
       " 'throughout',\n",
       " 'button',\n",
       " 'cams',\n",
       " 'dashboard',\n",
       " 'mirror',\n",
       " 'big',\n",
       " 'cuts',\n",
       " 'never',\n",
       " 'quite',\n",
       " 'sure',\n",
       " 'watching',\n",
       " 'camera',\n",
       " 'director',\n",
       " 'shows',\n",
       " 'omnipotent',\n",
       " 'creator',\n",
       " 'christof',\n",
       " 'ed',\n",
       " 'harris',\n",
       " 'oversees',\n",
       " 'control',\n",
       " 'room',\n",
       " 'built',\n",
       " 'moon',\n",
       " 'act',\n",
       " 'offers',\n",
       " 'plethora',\n",
       " 'clues',\n",
       " 'truth',\n",
       " 'existence',\n",
       " 'friend',\n",
       " 'marlon',\n",
       " 'noah',\n",
       " 'emmerich',\n",
       " 'always',\n",
       " 'six',\n",
       " 'pack',\n",
       " 'beer',\n",
       " 'exist',\n",
       " 'eternal',\n",
       " 'commercial',\n",
       " 'hot',\n",
       " 'household',\n",
       " 'product',\n",
       " 'sees',\n",
       " 'people',\n",
       " 'walking',\n",
       " 'set',\n",
       " 'patterns',\n",
       " 'wonders',\n",
       " 'didnt',\n",
       " 'pick',\n",
       " 'earlier',\n",
       " 'answer',\n",
       " 'tend',\n",
       " 'accept',\n",
       " 'reality',\n",
       " 'theyre',\n",
       " 'presented',\n",
       " 'nevertheless',\n",
       " 'begins',\n",
       " 'grow',\n",
       " 'restless',\n",
       " 'dreams',\n",
       " 'escape',\n",
       " 'fiji',\n",
       " 'old',\n",
       " 'college',\n",
       " 'sweetheart',\n",
       " 'natasha',\n",
       " 'mcelhone',\n",
       " 'supposedly',\n",
       " 'lives',\n",
       " 'convincing',\n",
       " 'progenitor',\n",
       " 'provocative',\n",
       " 'concept',\n",
       " 'thing',\n",
       " 'stay',\n",
       " 'way',\n",
       " 'allows',\n",
       " 'subtle',\n",
       " 'manipulations',\n",
       " 'particularly',\n",
       " 'poignant',\n",
       " 'scene',\n",
       " 'confides',\n",
       " 'fears',\n",
       " 'answers',\n",
       " 'bestfriend',\n",
       " 'sincerity',\n",
       " 'gladly',\n",
       " 'step',\n",
       " 'front',\n",
       " 'bus',\n",
       " 'line',\n",
       " 'fed',\n",
       " 'abject',\n",
       " 'cruelty',\n",
       " 'subjected',\n",
       " 'hits',\n",
       " 'home',\n",
       " 'moment',\n",
       " 'derives',\n",
       " 'much',\n",
       " 'success',\n",
       " 'playing',\n",
       " 'secret',\n",
       " 'paranoid',\n",
       " 'fantasies',\n",
       " 'havent',\n",
       " 'least',\n",
       " 'doubted',\n",
       " 'place',\n",
       " 'closest',\n",
       " 'us',\n",
       " 'ultimately',\n",
       " 'rises',\n",
       " 'artifice',\n",
       " 'raise',\n",
       " 'real',\n",
       " 'questions',\n",
       " 'relationship',\n",
       " 'humankind',\n",
       " 'god',\n",
       " 'think',\n",
       " 'resent',\n",
       " 'abandonment',\n",
       " 'paradise',\n",
       " 'exactly',\n",
       " 'credit',\n",
       " 'allowing',\n",
       " 'magic',\n",
       " 'screenplay',\n",
       " 'well',\n",
       " 'kind',\n",
       " 'role',\n",
       " 'jimmy',\n",
       " 'stewart',\n",
       " 'born',\n",
       " 'play',\n",
       " 'doesnt',\n",
       " 'try',\n",
       " 'tries',\n",
       " 'feel',\n",
       " 'suddenly',\n",
       " 'found',\n",
       " 'whole',\n",
       " 'usually',\n",
       " 'movie',\n",
       " 'soiled',\n",
       " 'rug',\n",
       " 'lebowski',\n",
       " 'new',\n",
       " 'offering',\n",
       " 'creators',\n",
       " 'critical',\n",
       " 'hit',\n",
       " 'fargo',\n",
       " 'wildly',\n",
       " 'entertaining',\n",
       " 'originality',\n",
       " 'strong',\n",
       " 'trait',\n",
       " 'coen',\n",
       " 'brothers',\n",
       " 'movies',\n",
       " 'insanely',\n",
       " 'original',\n",
       " 'oddly',\n",
       " 'enough',\n",
       " 'jeff',\n",
       " 'dude',\n",
       " 'bridges',\n",
       " 'mistaken',\n",
       " 'millionaire',\n",
       " 'david',\n",
       " 'huddleston',\n",
       " 'whos',\n",
       " 'bunny',\n",
       " 'tara',\n",
       " 'reid',\n",
       " 'owes',\n",
       " 'money',\n",
       " 'two',\n",
       " 'goons',\n",
       " 'collect',\n",
       " 'bunnys',\n",
       " 'debt',\n",
       " 'break',\n",
       " 'dudes',\n",
       " 'attempt',\n",
       " 'urinates',\n",
       " 'next',\n",
       " 'explains',\n",
       " 'situation',\n",
       " 'bowling',\n",
       " 'team',\n",
       " 'steve',\n",
       " 'buscemi',\n",
       " 'john',\n",
       " 'goodman',\n",
       " 'respectively',\n",
       " 'advised',\n",
       " 'go',\n",
       " 'get',\n",
       " 'pay',\n",
       " 'defiled',\n",
       " 'shortly',\n",
       " 'gets',\n",
       " 'tied',\n",
       " 'kidnapping',\n",
       " 'caper',\n",
       " 'extremely',\n",
       " 'funny',\n",
       " 'lots',\n",
       " 'belly',\n",
       " 'laugh',\n",
       " 'moments',\n",
       " 'assembled',\n",
       " 'great',\n",
       " 'cast',\n",
       " 'perfectly',\n",
       " 'also',\n",
       " 'several',\n",
       " 'supporting',\n",
       " 'roles',\n",
       " 'turturro',\n",
       " 'child',\n",
       " 'molesting',\n",
       " 'bowler',\n",
       " 'named',\n",
       " 'jesus',\n",
       " 'coens',\n",
       " 'script',\n",
       " 'plays',\n",
       " 'goes',\n",
       " 'together',\n",
       " 'nearly',\n",
       " 'wall',\n",
       " 'anything',\n",
       " 'else',\n",
       " 'done',\n",
       " 'love',\n",
       " 'give',\n",
       " 'different',\n",
       " 'elizabeth',\n",
       " 'potent',\n",
       " 'historical',\n",
       " 'drama',\n",
       " 'england',\n",
       " 'royal',\n",
       " 'ceremonies',\n",
       " 'commonplace',\n",
       " 'public',\n",
       " 'internally',\n",
       " 'catholics',\n",
       " 'wage',\n",
       " 'war',\n",
       " 'protestants',\n",
       " 'meanwhile',\n",
       " 'spain',\n",
       " 'scotland',\n",
       " 'france',\n",
       " 'strategize',\n",
       " 'moves',\n",
       " 'struggle',\n",
       " 'power',\n",
       " 'young',\n",
       " 'woman',\n",
       " 'cate',\n",
       " 'blanchett',\n",
       " 'greatest',\n",
       " 'hope',\n",
       " 'survival',\n",
       " 'charts',\n",
       " 'elizabeths',\n",
       " 'tumultuous',\n",
       " 'gain',\n",
       " 'true',\n",
       " 'kingdom',\n",
       " 'protestant',\n",
       " 'person',\n",
       " 'catholic',\n",
       " 'royalty',\n",
       " 'want',\n",
       " 'queen',\n",
       " 'mary',\n",
       " 'kathy',\n",
       " 'burke',\n",
       " 'ill',\n",
       " 'unable',\n",
       " 'conceive',\n",
       " 'throne',\n",
       " 'gives',\n",
       " 'blessing',\n",
       " 'condition',\n",
       " 'renounce',\n",
       " 'faith',\n",
       " 'uphold',\n",
       " 'catholicism',\n",
       " 'across',\n",
       " 'land',\n",
       " 'declared',\n",
       " 'immediately',\n",
       " 'finds',\n",
       " 'assault',\n",
       " 'subjects',\n",
       " 'including',\n",
       " 'duke',\n",
       " 'played',\n",
       " 'grace',\n",
       " 'christopher',\n",
       " 'eccleston',\n",
       " 'forces',\n",
       " 'abroad',\n",
       " 'slowly',\n",
       " 'surely',\n",
       " 'neophyte',\n",
       " 'ruler',\n",
       " 'takes',\n",
       " 'reigns',\n",
       " 'forges',\n",
       " 'path',\n",
       " 'surrounded',\n",
       " 'keenly',\n",
       " 'cadre',\n",
       " 'advisers',\n",
       " 'ally',\n",
       " 'mysterious',\n",
       " 'sir',\n",
       " 'francis',\n",
       " 'geoffrey',\n",
       " 'rush',\n",
       " 'performance',\n",
       " 'william',\n",
       " 'cecil',\n",
       " 'richard',\n",
       " 'attenborough',\n",
       " 'wellmeaning',\n",
       " 'misguided',\n",
       " 'chief',\n",
       " 'adviser',\n",
       " 'trust',\n",
       " 'must',\n",
       " 'prove',\n",
       " 'selfworth',\n",
       " 'even',\n",
       " 'means',\n",
       " 'secretly',\n",
       " 'despises',\n",
       " 'deserves',\n",
       " 'oscar',\n",
       " 'portrays',\n",
       " 'right',\n",
       " 'balance',\n",
       " 'selfconsciousness',\n",
       " 'shrewd',\n",
       " 'charisma',\n",
       " 'though',\n",
       " 'classical',\n",
       " 'beauty',\n",
       " 'able',\n",
       " 'entrance',\n",
       " 'viewer',\n",
       " 'smile',\n",
       " 'impish',\n",
       " 'smirk',\n",
       " 'characters',\n",
       " 'playful',\n",
       " 'tendencies',\n",
       " 'dances',\n",
       " 'unabashedly',\n",
       " 'childhood',\n",
       " 'lover',\n",
       " 'lord',\n",
       " 'robert',\n",
       " 'dudley',\n",
       " 'joseph',\n",
       " 'fiennes',\n",
       " 'whether',\n",
       " 'unctuous',\n",
       " 'suitors',\n",
       " 'houses',\n",
       " 'court',\n",
       " 'blanchetts',\n",
       " 'radiates',\n",
       " 'confidence',\n",
       " 'impossible',\n",
       " 'dislike',\n",
       " 'almost',\n",
       " 'character',\n",
       " 'visual',\n",
       " 'delights',\n",
       " 'cinematographer',\n",
       " 'remi',\n",
       " 'adefarasin',\n",
       " 'crafted',\n",
       " 'rich',\n",
       " 'color',\n",
       " 'palette',\n",
       " 'feast',\n",
       " 'eyes',\n",
       " 'rolling',\n",
       " 'green',\n",
       " 'hills',\n",
       " 'extravagant',\n",
       " 'ceremonial',\n",
       " 'dark',\n",
       " 'foreboding',\n",
       " 'corridors',\n",
       " 'depicted',\n",
       " 'sense',\n",
       " 'artistic',\n",
       " 'appreciation',\n",
       " 'castles',\n",
       " 'europe',\n",
       " 'rarely',\n",
       " 'doted',\n",
       " 'lovingly',\n",
       " 'writer',\n",
       " 'murky',\n",
       " 'times',\n",
       " 'humor',\n",
       " 'overplayed',\n",
       " 'amount',\n",
       " 'dramatic',\n",
       " 'cinematic',\n",
       " 'verve',\n",
       " 'deserving',\n",
       " 'theatrical',\n",
       " 'viewing',\n",
       " 'story',\n",
       " 'survives',\n",
       " 'attention',\n",
       " 'shocking',\n",
       " 'sensible',\n",
       " 'final',\n",
       " 'declaration',\n",
       " 'ready',\n",
       " 'bow',\n",
       " 'certain',\n",
       " 'talent',\n",
       " 'chose',\n",
       " 'advantage',\n",
       " 'personal',\n",
       " 'reason',\n",
       " 'hunting',\n",
       " 'title',\n",
       " 'lets',\n",
       " 'face',\n",
       " 'bitch',\n",
       " 'cowriter',\n",
       " 'matt',\n",
       " 'damon',\n",
       " 'mathematical',\n",
       " 'genius',\n",
       " 'brew',\n",
       " 'cup',\n",
       " 'irish',\n",
       " 'cream',\n",
       " 'works',\n",
       " 'mit',\n",
       " 'janitor',\n",
       " 'professor',\n",
       " 'challenging',\n",
       " 'courses',\n",
       " 'proffessor',\n",
       " 'lambeau',\n",
       " 'breaking',\n",
       " 'waves',\n",
       " 'stellan',\n",
       " 'skarsg',\n",
       " 'rd',\n",
       " 'puts',\n",
       " 'problem',\n",
       " 'board',\n",
       " 'students',\n",
       " 'claims',\n",
       " 'another',\n",
       " 'took',\n",
       " 'colleagues',\n",
       " 'years',\n",
       " 'duh',\n",
       " 'course',\n",
       " 'catch',\n",
       " 'guy',\n",
       " 'red',\n",
       " 'handed',\n",
       " 'find',\n",
       " 'ran',\n",
       " 'away',\n",
       " 'caught',\n",
       " 'gotten',\n",
       " 'fight',\n",
       " 'bunch',\n",
       " 'punks',\n",
       " 'basketball',\n",
       " 'struck',\n",
       " 'police',\n",
       " 'officer',\n",
       " 'probation',\n",
       " 'math',\n",
       " 'therapy',\n",
       " 'going',\n",
       " 'therapists',\n",
       " 'psyches',\n",
       " 'fakes',\n",
       " 'spell',\n",
       " 'launches',\n",
       " 'impromptu',\n",
       " 'classic',\n",
       " 'afternoon',\n",
       " 'delight',\n",
       " 'finally',\n",
       " 'lambeaus',\n",
       " 'sean',\n",
       " 'mcguire',\n",
       " 'robin',\n",
       " 'williams',\n",
       " 'psych',\n",
       " 'agrees',\n",
       " 'treat',\n",
       " 'begin',\n",
       " 'rocky',\n",
       " 'open',\n",
       " 'turns',\n",
       " 'rough',\n",
       " 'abandoned',\n",
       " 'placed',\n",
       " 'foster',\n",
       " 'homes',\n",
       " 'simple',\n",
       " 'mathematics',\n",
       " 'wants',\n",
       " 'challenge',\n",
       " 'hang',\n",
       " 'ben',\n",
       " 'affleck',\n",
       " 'honorable',\n",
       " 'jobs',\n",
       " 'construction',\n",
       " 'follow',\n",
       " 'footsteps',\n",
       " 'einstein',\n",
       " 'forced',\n",
       " 'appears',\n",
       " 'million',\n",
       " 'jampacked',\n",
       " 'greatness',\n",
       " 'totally',\n",
       " 'brilliant',\n",
       " 'im',\n",
       " 'saying',\n",
       " 'bad',\n",
       " 'tons',\n",
       " 'things',\n",
       " 'acting',\n",
       " 'amazing',\n",
       " 'performances',\n",
       " 'chemistry',\n",
       " 'support',\n",
       " 'name',\n",
       " 'minnie',\n",
       " 'driver',\n",
       " 'wills',\n",
       " 'interest',\n",
       " 'skylar',\n",
       " 'individual',\n",
       " 'adored',\n",
       " 'loved',\n",
       " 'comedy',\n",
       " 'especially',\n",
       " 'proves',\n",
       " 'schmuck',\n",
       " 'student',\n",
       " 'quoting',\n",
       " 'historian',\n",
       " 'impress',\n",
       " 'chicks',\n",
       " 'various',\n",
       " 'attempts',\n",
       " 'favorite',\n",
       " 'entire',\n",
       " 'delivers',\n",
       " 'long',\n",
       " 'rant',\n",
       " 'nsa',\n",
       " 'agents',\n",
       " 'downside',\n",
       " 'liked',\n",
       " 'scenes',\n",
       " 'honest',\n",
       " 'portrayal',\n",
       " 'rightfully',\n",
       " 'uncomfortable',\n",
       " 'breakup',\n",
       " 'seans',\n",
       " 'meeting',\n",
       " 'bar',\n",
       " 'tells',\n",
       " 'detail',\n",
       " 'regret',\n",
       " 'later',\n",
       " 'suffer',\n",
       " 'slow',\n",
       " 'painful',\n",
       " 'death',\n",
       " 'leave',\n",
       " 'lonely',\n",
       " 'slightly',\n",
       " 'bitter',\n",
       " 'main',\n",
       " 'flaw',\n",
       " 'theres',\n",
       " 'everything',\n",
       " 'either',\n",
       " 'deeper',\n",
       " 'relations',\n",
       " 'overwritten',\n",
       " 'underwritten',\n",
       " 'example',\n",
       " 'fatherson',\n",
       " 'problems',\n",
       " 'equals',\n",
       " 'hear',\n",
       " 'went',\n",
       " 'hypocritical',\n",
       " 'parts',\n",
       " 'chuckie',\n",
       " 'nice',\n",
       " 'climax',\n",
       " 'rising',\n",
       " 'action',\n",
       " 'joking',\n",
       " 'around',\n",
       " 'arrives',\n",
       " 'decision',\n",
       " 'superficial',\n",
       " 'reasoning',\n",
       " 'selected',\n",
       " 'beyond',\n",
       " 'instead',\n",
       " 'human',\n",
       " 'maybe',\n",
       " 'films',\n",
       " 'pretty',\n",
       " 'understand',\n",
       " 'confesses',\n",
       " 'loves',\n",
       " 'details',\n",
       " 'breathes',\n",
       " 'writing',\n",
       " 'bit',\n",
       " 'fault',\n",
       " 'worthwhile',\n",
       " 'feels',\n",
       " 'overstuffed',\n",
       " 'dialogue',\n",
       " 'written',\n",
       " 'amazingly',\n",
       " 'fresh',\n",
       " 'nomination',\n",
       " 'chiefly',\n",
       " 'gus',\n",
       " 'van',\n",
       " 'sants',\n",
       " 'conservative',\n",
       " 'youd',\n",
       " 'hardly',\n",
       " 'die',\n",
       " 'sant',\n",
       " 'ive',\n",
       " 'seen',\n",
       " 'still',\n",
       " 'remarkable',\n",
       " 'albeit',\n",
       " 'tad',\n",
       " 'overrated',\n",
       " 'whats',\n",
       " 'deal',\n",
       " 'despite',\n",
       " 'lot',\n",
       " 'flaws',\n",
       " 'anyone',\n",
       " 'victims',\n",
       " 'fate',\n",
       " 'create',\n",
       " 'destiny',\n",
       " 'directorwriter',\n",
       " 'brad',\n",
       " 'anderson',\n",
       " 'yes',\n",
       " 'witty',\n",
       " 'stop',\n",
       " 'wonderland',\n",
       " 'alan',\n",
       " 'erin',\n",
       " 'spend',\n",
       " 'others',\n",
       " 'orbit',\n",
       " 'catching',\n",
       " 'glimpses',\n",
       " 'connecting',\n",
       " 'inevitable',\n",
       " 'conclusion',\n",
       " 'keeps',\n",
       " 'near',\n",
       " 'carry',\n",
       " 'aspects',\n",
       " 'starts',\n",
       " 'davis',\n",
       " 'melancholy',\n",
       " 'late',\n",
       " 'shift',\n",
       " 'nurse',\n",
       " 'ending',\n",
       " 'livein',\n",
       " 'boyfriend',\n",
       " 'hoffman',\n",
       " 'rather',\n",
       " 'arriving',\n",
       " 'parked',\n",
       " 'apartment',\n",
       " 'car',\n",
       " 'packed',\n",
       " 'belongings',\n",
       " 'fumbling',\n",
       " 'amusing',\n",
       " 'self',\n",
       " 'ramble',\n",
       " 'instructs',\n",
       " 'videotape',\n",
       " 'hes',\n",
       " 'made',\n",
       " 'detailing',\n",
       " 'doomed',\n",
       " 'fail',\n",
       " 'obvious',\n",
       " 'lacks',\n",
       " 'courage',\n",
       " 'confront',\n",
       " 'directly',\n",
       " 'rails',\n",
       " 'taking',\n",
       " 'stand',\n",
       " 'flees',\n",
       " 'inept',\n",
       " 'thief',\n",
       " 'erins',\n",
       " 'mother',\n",
       " 'holland',\n",
       " 'taylor',\n",
       " 'surprisingly',\n",
       " 'effective',\n",
       " 'fearing',\n",
       " 'daughter',\n",
       " 'without',\n",
       " 'man',\n",
       " 'ad',\n",
       " 'embarrassed',\n",
       " 'horror',\n",
       " 'describes',\n",
       " 'frisky',\n",
       " 'cultured',\n",
       " 'carefree',\n",
       " 'professional',\n",
       " 'zest',\n",
       " 'eventually',\n",
       " 'responds',\n",
       " 'tidal',\n",
       " 'wave',\n",
       " 'responses',\n",
       " 'humorous',\n",
       " 'telling',\n",
       " 'meets',\n",
       " 'prospective',\n",
       " 'poseurs',\n",
       " 'plumber',\n",
       " 'aspiring',\n",
       " 'marine',\n",
       " 'biologist',\n",
       " 'spots',\n",
       " 'cleaning',\n",
       " 'inside',\n",
       " 'glass',\n",
       " 'fish',\n",
       " 'tank',\n",
       " 'boston',\n",
       " 'aquarium',\n",
       " 'wearing',\n",
       " 'wet',\n",
       " 'suit',\n",
       " 'goggles',\n",
       " 'follows',\n",
       " 'window',\n",
       " 'separated',\n",
       " 'oblivious',\n",
       " 'gaze',\n",
       " 'enjoys',\n",
       " 'train',\n",
       " 'sits',\n",
       " 'platform',\n",
       " 'outside',\n",
       " 'mere',\n",
       " 'feet',\n",
       " 'spends',\n",
       " 'entirety',\n",
       " 'paths',\n",
       " 'circle',\n",
       " 'crossing',\n",
       " 'subplots',\n",
       " 'involving',\n",
       " 'concerns',\n",
       " 'job',\n",
       " ...]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = open('datasets/vocab.txt')\n",
    "vocab = f.read().split()\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4a64a5d2-618a-4800-9285-94b73fac9e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the file, clean the data and return lines\n",
    "def doc_to_line(filename):\n",
    "    doc = load_doc(filename)\n",
    "    tokens = clean_doc(doc)\n",
    "    tokens = [token for token in tokens if token in vocab]\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "00ccbab6-341b-4581-adaa-2d2cf4c5ff64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'films adapted comic books plenty success whether superheroes batman superman spawn geared toward kids casper arthouse crowd ghost world never really comic book like hell starters created alan moore eddie campbell brought medium whole new level mid series called say moore campbell thoroughly subject jack ripper would like saying michael jackson starting look little odd book graphic novel pages long includes nearly consist nothing footnotes words dismiss film source get past whole comic book thing might find another stumbling block hell directors albert allen hughes getting hughes brothers direct seems almost ludicrous casting carrot top well anything better direct film set ghetto features really violent street crime mad geniuses behind menace ii society ghetto question course whitechapel london east end filthy place whores called unfortunates starting get little nervous mysterious psychopath carving profession surgical precision first stiff turns copper peter robbie coltrane world enough calls inspector frederick abberline johnny depp blow crack case abberline widower prophetic dreams unsuccessfully tries quell copious amounts absinthe opium upon arriving whitechapel befriends unfortunate named mary kelly heather graham say proceeds investigate horribly gruesome crimes even police surgeon ca stomach think anyone needs jack ripper wo go particulars say moore campbell unique interesting theory identity killer reasons chooses slay comic bother identity ripper screenwriters terry hayes vertical limit rafael les mis rables good job keeping hidden viewers end funny watch locals blindly point finger blame jews indians englishman could never capable committing ghastly acts hell ending song simpsons days holds back electric made steve guttenberg star worry make sense see onto hell appearance certainly dark bleak enough surprising see much looks like tim burton film planet apes times seems like sleepy hollow print saw completely finished color music finalized comments marilyn manson cinematographer peter say word ably captures dreariness london helped make flashy killing scenes remind crazy flashbacks twin peaks even though violence film pales comparison comic oscar winner martin childs shakespeare love production design turns original surroundings one creepy place even acting hell solid dreamy depp turning typically strong performance deftly handling british accent holm joe gould secret richardson dalmatians log great supporting roles big surprise graham cringed first time opened mouth imagining attempt irish accent actually half bad film however good strong sexuality language drug content'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_to_line('datasets/review_polarity/txt_sentoken/pos/cv000_29590.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "68b4eb2c-90a4-415f-9320-bad979edf5c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cv887_5126.txt',\n",
       " 'cv192_14395.txt',\n",
       " 'cv083_24234.txt',\n",
       " 'cv426_10421.txt',\n",
       " 'cv155_7308.txt',\n",
       " 'cv627_11620.txt',\n",
       " 'cv068_13400.txt',\n",
       " 'cv232_14991.txt',\n",
       " 'cv120_4111.txt',\n",
       " 'cv738_10116.txt',\n",
       " 'cv029_18643.txt',\n",
       " 'cv661_2450.txt',\n",
       " 'cv302_25649.txt',\n",
       " 'cv210_9312.txt',\n",
       " 'cv857_15958.txt',\n",
       " 'cv675_21588.txt',\n",
       " 'cv711_11665.txt',\n",
       " 'cv771_28665.txt',\n",
       " 'cv527_10123.txt',\n",
       " 'cv568_15638.txt',\n",
       " 'cv187_12829.txt',\n",
       " 'cv769_8123.txt',\n",
       " 'cv427_10825.txt',\n",
       " 'cv972_26417.txt',\n",
       " 'cv903_17822.txt',\n",
       " 'cv651_10492.txt',\n",
       " 'cv792_3832.txt',\n",
       " 'cv450_7890.txt',\n",
       " 'cv863_7424.txt',\n",
       " 'cv764_11739.txt',\n",
       " 'cv332_16307.txt',\n",
       " 'cv982_21103.txt',\n",
       " 'cv231_10425.txt',\n",
       " 'cv321_12843.txt',\n",
       " 'cv664_4389.txt',\n",
       " 'cv772_12119.txt',\n",
       " 'cv076_24945.txt',\n",
       " 'cv715_18179.txt',\n",
       " 'cv918_2693.txt',\n",
       " 'cv445_25882.txt',\n",
       " 'cv375_9929.txt',\n",
       " 'cv418_14774.txt',\n",
       " 'cv247_13142.txt',\n",
       " 'cv414_10518.txt',\n",
       " 'cv182_7281.txt',\n",
       " 'cv804_10862.txt',\n",
       " 'cv783_13227.txt',\n",
       " 'cv500_10251.txt',\n",
       " 'cv158_10390.txt',\n",
       " 'cv999_13106.txt',\n",
       " 'cv536_27134.txt',\n",
       " 'cv731_4136.txt',\n",
       " 'cv460_10842.txt',\n",
       " 'cv200_2915.txt',\n",
       " 'cv190_27052.txt',\n",
       " 'cv812_17924.txt',\n",
       " 'cv250_25616.txt',\n",
       " 'cv694_4876.txt',\n",
       " 'cv487_10446.txt',\n",
       " 'cv701_14252.txt',\n",
       " 'cv577_28549.txt',\n",
       " 'cv441_13711.txt',\n",
       " 'cv498_8832.txt',\n",
       " 'cv407_22637.txt',\n",
       " 'cv773_18817.txt',\n",
       " 'cv689_12587.txt',\n",
       " 'cv947_10601.txt',\n",
       " 'cv765_19037.txt',\n",
       " 'cv442_13846.txt',\n",
       " 'cv808_12635.txt',\n",
       " 'cv682_16139.txt',\n",
       " 'cv239_3385.txt',\n",
       " 'cv646_15065.txt',\n",
       " 'cv870_16348.txt',\n",
       " 'cv009_29592.txt',\n",
       " 'cv967_5788.txt',\n",
       " 'cv327_20292.txt',\n",
       " 'cv609_23877.txt',\n",
       " 'cv070_12289.txt',\n",
       " 'cv519_14661.txt',\n",
       " 'cv291_26635.txt',\n",
       " 'cv550_22211.txt',\n",
       " 'cv793_13650.txt',\n",
       " 'cv659_19944.txt',\n",
       " 'cv669_22995.txt',\n",
       " 'cv062_23115.txt',\n",
       " 'cv213_18934.txt',\n",
       " 'cv861_1198.txt',\n",
       " 'cv049_20471.txt',\n",
       " 'cv587_19162.txt',\n",
       " 'cv329_29370.txt',\n",
       " 'cv161_11425.txt',\n",
       " 'cv077_22138.txt',\n",
       " 'cv274_25253.txt',\n",
       " 'cv607_7717.txt',\n",
       " 'cv261_10954.txt',\n",
       " 'cv126_28971.txt',\n",
       " 'cv116_28942.txt',\n",
       " 'cv494_17389.txt',\n",
       " 'cv742_7751.txt',\n",
       " 'cv084_13566.txt',\n",
       " 'cv736_23670.txt',\n",
       " 'cv470_15952.txt',\n",
       " 'cv280_8267.txt',\n",
       " 'cv653_19583.txt',\n",
       " 'cv759_13522.txt',\n",
       " 'cv700_21947.txt',\n",
       " 'cv914_28742.txt',\n",
       " 'cv240_14336.txt',\n",
       " 'cv102_7846.txt',\n",
       " 'cv746_10147.txt',\n",
       " 'cv347_13194.txt',\n",
       " 'cv785_22600.txt',\n",
       " 'cv893_26269.txt',\n",
       " 'cv872_12591.txt',\n",
       " 'cv768_11751.txt',\n",
       " 'cv160_10362.txt',\n",
       " 'cv540_3421.txt',\n",
       " 'cv339_21119.txt',\n",
       " 'cv781_5262.txt',\n",
       " 'cv003_11664.txt',\n",
       " 'cv564_11110.txt',\n",
       " 'cv364_12901.txt',\n",
       " 'cv293_29856.txt',\n",
       " 'cv071_12095.txt',\n",
       " 'cv451_10690.txt',\n",
       " 'cv252_23779.txt',\n",
       " 'cv938_10220.txt',\n",
       " 'cv023_12672.txt',\n",
       " 'cv249_11640.txt',\n",
       " 'cv645_15668.txt',\n",
       " 'cv174_9659.txt',\n",
       " 'cv359_6647.txt',\n",
       " 'cv081_16582.txt',\n",
       " 'cv685_5947.txt',\n",
       " 'cv437_22849.txt',\n",
       " 'cv610_2287.txt',\n",
       " 'cv991_18645.txt',\n",
       " 'cv379_21963.txt',\n",
       " 'cv532_6522.txt',\n",
       " 'cv819_9364.txt',\n",
       " 'cv036_16831.txt',\n",
       " 'cv027_25219.txt',\n",
       " 'cv473_7367.txt',\n",
       " 'cv429_7439.txt',\n",
       " 'cv884_13632.txt',\n",
       " 'cv404_20315.txt',\n",
       " 'cv509_15888.txt',\n",
       " 'cv006_15448.txt',\n",
       " 'cv766_7540.txt',\n",
       " 'cv507_9220.txt',\n",
       " 'cv417_13115.txt',\n",
       " 'cv932_13401.txt',\n",
       " 'cv481_7436.txt',\n",
       " 'cv333_8916.txt',\n",
       " 'cv634_11101.txt',\n",
       " 'cv286_25050.txt',\n",
       " 'cv367_22792.txt',\n",
       " 'cv403_6621.txt',\n",
       " 'cv565_29572.txt',\n",
       " 'cv913_29252.txt',\n",
       " 'cv734_21568.txt',\n",
       " 'cv831_14689.txt',\n",
       " 'cv790_14600.txt',\n",
       " 'cv865_2895.txt',\n",
       " 'cv135_11603.txt',\n",
       " 'cv940_17705.txt',\n",
       " 'cv723_8648.txt',\n",
       " 'cv408_5297.txt',\n",
       " 'cv479_5649.txt',\n",
       " 'cv164_22447.txt',\n",
       " 'cv842_5866.txt',\n",
       " 'cv115_25396.txt',\n",
       " 'cv456_18985.txt',\n",
       " 'cv706_24716.txt',\n",
       " 'cv580_14064.txt',\n",
       " 'cv696_29740.txt',\n",
       " 'cv045_23923.txt',\n",
       " 'cv198_18180.txt',\n",
       " 'cv880_29800.txt',\n",
       " 'cv988_18740.txt',\n",
       " 'cv602_8300.txt',\n",
       " 'cv894_2068.txt',\n",
       " 'cv847_1941.txt',\n",
       " 'cv604_2230.txt',\n",
       " 'cv166_11052.txt',\n",
       " 'cv288_18791.txt',\n",
       " 'cv259_10934.txt',\n",
       " 'cv489_17906.txt',\n",
       " 'cv574_22156.txt',\n",
       " 'cv384_17140.txt',\n",
       " 'cv304_28706.txt',\n",
       " 'cv749_17765.txt',\n",
       " 'cv209_29118.txt',\n",
       " 'cv015_29439.txt',\n",
       " 'cv092_28017.txt',\n",
       " 'cv376_19435.txt',\n",
       " 'cv391_10802.txt',\n",
       " 'cv131_10713.txt',\n",
       " 'cv238_12931.txt',\n",
       " 'cv117_24295.txt',\n",
       " 'cv127_14711.txt',\n",
       " 'cv305_9946.txt',\n",
       " 'cv199_9629.txt',\n",
       " 'cv817_4041.txt',\n",
       " 'cv335_14665.txt',\n",
       " 'cv598_16452.txt',\n",
       " 'cv467_25773.txt',\n",
       " 'cv227_24215.txt',\n",
       " 'cv740_12445.txt',\n",
       " 'cv531_26486.txt',\n",
       " 'cv751_15719.txt',\n",
       " 'cv515_17069.txt',\n",
       " 'cv336_10143.txt',\n",
       " 'cv636_15279.txt',\n",
       " 'cv254_6027.txt',\n",
       " 'cv204_8451.txt',\n",
       " 'cv285_16494.txt',\n",
       " 'cv273_29112.txt',\n",
       " 'cv552_10016.txt',\n",
       " 'cv272_18974.txt',\n",
       " 'cv678_13419.txt',\n",
       " 'cv674_10732.txt',\n",
       " 'cv208_9020.txt',\n",
       " 'cv343_10368.txt',\n",
       " 'cv578_15094.txt',\n",
       " 'cv633_29837.txt',\n",
       " 'cv922_10073.txt',\n",
       " 'cv035_3954.txt',\n",
       " 'cv980_10953.txt',\n",
       " 'cv011_12166.txt',\n",
       " 'cv034_29647.txt',\n",
       " 'cv201_6997.txt',\n",
       " 'cv642_29867.txt',\n",
       " 'cv673_24714.txt',\n",
       " 'cv905_29114.txt',\n",
       " 'cv152_8736.txt',\n",
       " 'cv263_19259.txt',\n",
       " 'cv941_10246.txt',\n",
       " 'cv695_21108.txt',\n",
       " 'cv172_11131.txt',\n",
       " 'cv892_17576.txt',\n",
       " 'cv753_10875.txt',\n",
       " 'cv518_13331.txt',\n",
       " 'cv533_9821.txt',\n",
       " 'cv133_16336.txt',\n",
       " 'cv474_10209.txt',\n",
       " 'cv361_28944.txt',\n",
       " 'cv430_17351.txt',\n",
       " 'cv958_12162.txt',\n",
       " 'cv585_22496.txt',\n",
       " 'cv943_22488.txt',\n",
       " 'cv795_10122.txt',\n",
       " 'cv181_14401.txt',\n",
       " 'cv216_18738.txt',\n",
       " 'cv882_10026.txt',\n",
       " 'cv594_11039.txt',\n",
       " 'cv350_20670.txt',\n",
       " 'cv719_5713.txt',\n",
       " 'cv093_13951.txt',\n",
       " 'cv374_25436.txt',\n",
       " 'cv712_22920.txt',\n",
       " 'cv925_8969.txt',\n",
       " 'cv338_8821.txt',\n",
       " 'cv000_29590.txt',\n",
       " 'cv658_10532.txt',\n",
       " 'cv245_8569.txt',\n",
       " 'cv046_10188.txt',\n",
       " 'cv960_29007.txt',\n",
       " 'cv103_11021.txt',\n",
       " 'cv019_14482.txt',\n",
       " 'cv779_17881.txt',\n",
       " 'cv078_14730.txt',\n",
       " 'cv354_8132.txt',\n",
       " 'cv275_28887.txt',\n",
       " 'cv212_10027.txt',\n",
       " 'cv244_21649.txt',\n",
       " 'cv067_19774.txt',\n",
       " 'cv214_12294.txt',\n",
       " 'cv803_8207.txt',\n",
       " 'cv118_28980.txt',\n",
       " 'cv180_16113.txt',\n",
       " 'cv357_13156.txt',\n",
       " 'cv351_15458.txt',\n",
       " 'cv630_10057.txt',\n",
       " 'cv688_7368.txt',\n",
       " 'cv758_9671.txt',\n",
       " 'cv816_13655.txt',\n",
       " 'cv686_13900.txt',\n",
       " 'cv362_15341.txt',\n",
       " 'cv119_9867.txt',\n",
       " 'cv478_14309.txt',\n",
       " 'cv511_10132.txt',\n",
       " 'cv221_2695.txt',\n",
       " 'cv827_18331.txt',\n",
       " 'cv503_10558.txt',\n",
       " 'cv909_9960.txt',\n",
       " 'cv358_10691.txt',\n",
       " 'cv573_29525.txt',\n",
       " 'cv949_20112.txt',\n",
       " 'cv647_13691.txt',\n",
       " 'cv907_3541.txt',\n",
       " 'cv353_18159.txt',\n",
       " 'cv276_15684.txt',\n",
       " 'cv091_7400.txt',\n",
       " 'cv983_22928.txt',\n",
       " 'cv878_15694.txt',\n",
       " 'cv869_23611.txt',\n",
       " 'cv415_22517.txt',\n",
       " 'cv541_28835.txt',\n",
       " 'cv815_22456.txt',\n",
       " 'cv060_10844.txt',\n",
       " 'cv410_24266.txt',\n",
       " 'cv168_7050.txt',\n",
       " 'cv241_23130.txt',\n",
       " 'cv282_6653.txt',\n",
       " 'cv929_16908.txt',\n",
       " 'cv599_20988.txt',\n",
       " 'cv876_9390.txt',\n",
       " 'cv569_26381.txt',\n",
       " 'cv964_6021.txt',\n",
       " 'cv097_24970.txt',\n",
       " 'cv858_18819.txt',\n",
       " 'cv910_20488.txt',\n",
       " 'cv639_10308.txt',\n",
       " 'cv891_6385.txt',\n",
       " 'cv974_22941.txt',\n",
       " 'cv592_22315.txt',\n",
       " 'cv256_14740.txt',\n",
       " 'cv306_10364.txt',\n",
       " 'cv425_8250.txt',\n",
       " 'cv025_3108.txt',\n",
       " 'cv670_25826.txt',\n",
       " 'cv789_12136.txt',\n",
       " 'cv963_6895.txt',\n",
       " 'cv987_6965.txt',\n",
       " 'cv848_10036.txt',\n",
       " 'cv153_10779.txt',\n",
       " 'cv348_18176.txt',\n",
       " 'cv802_28664.txt',\n",
       " 'cv106_16807.txt',\n",
       " 'cv703_16143.txt',\n",
       " 'cv836_12968.txt',\n",
       " 'cv745_12773.txt',\n",
       " 'cv722_7110.txt',\n",
       " 'cv146_18458.txt',\n",
       " 'cv978_20929.txt',\n",
       " 'cv825_5063.txt',\n",
       " 'cv506_15956.txt',\n",
       " 'cv145_11472.txt',\n",
       " 'cv562_10359.txt',\n",
       " 'cv326_13295.txt',\n",
       " 'cv590_19290.txt',\n",
       " 'cv823_15569.txt',\n",
       " 'cv702_11500.txt',\n",
       " 'cv394_5137.txt',\n",
       " 'cv788_25272.txt',\n",
       " 'cv890_3977.txt',\n",
       " 'cv207_29284.txt',\n",
       " 'cv549_21443.txt',\n",
       " 'cv998_14111.txt',\n",
       " 'cv040_8276.txt',\n",
       " 'cv363_29332.txt',\n",
       " 'cv477_22479.txt',\n",
       " 'cv228_5806.txt',\n",
       " 'cv022_12864.txt',\n",
       " 'cv955_25001.txt',\n",
       " 'cv053_21822.txt',\n",
       " 'cv730_10279.txt',\n",
       " 'cv747_16556.txt',\n",
       " 'cv707_10678.txt',\n",
       " 'cv864_3416.txt',\n",
       " 'cv283_11055.txt',\n",
       " 'cv400_19220.txt',\n",
       " 'cv668_17604.txt',\n",
       " 'cv325_16629.txt',\n",
       " 'cv845_14290.txt',\n",
       " 'cv463_10343.txt',\n",
       " 'cv561_9201.txt',\n",
       " 'cv826_11834.txt',\n",
       " 'cv382_7897.txt',\n",
       " 'cv581_19381.txt',\n",
       " 'cv057_7453.txt',\n",
       " 'cv649_12735.txt',\n",
       " 'cv791_16302.txt',\n",
       " 'cv296_12251.txt',\n",
       " 'cv121_17302.txt',\n",
       " 'cv551_10565.txt',\n",
       " 'cv737_28907.txt',\n",
       " 'cv421_9709.txt',\n",
       " 'cv716_10514.txt',\n",
       " 'cv233_15964.txt',\n",
       " 'cv482_10580.txt',\n",
       " 'cv148_16345.txt',\n",
       " 'cv951_10926.txt',\n",
       " 'cv624_10744.txt',\n",
       " 'cv346_18168.txt',\n",
       " 'cv381_20172.txt',\n",
       " 'cv835_19159.txt',\n",
       " 'cv859_14107.txt',\n",
       " 'cv433_10144.txt',\n",
       " 'cv923_11051.txt',\n",
       " 'cv575_21150.txt',\n",
       " 'cv871_24888.txt',\n",
       " 'cv013_10159.txt',\n",
       " 'cv614_10626.txt',\n",
       " 'cv596_28311.txt',\n",
       " 'cv966_28832.txt',\n",
       " 'cv652_13972.txt',\n",
       " 'cv125_9391.txt',\n",
       " 'cv349_13507.txt',\n",
       " 'cv697_11162.txt',\n",
       " 'cv389_9369.txt',\n",
       " 'cv104_18134.txt',\n",
       " 'cv535_19728.txt',\n",
       " 'cv877_29274.txt',\n",
       " 'cv048_16828.txt',\n",
       " 'cv660_21893.txt',\n",
       " 'cv007_4968.txt',\n",
       " 'cv916_15467.txt',\n",
       " 'cv392_11458.txt',\n",
       " 'cv188_19226.txt',\n",
       " 'cv085_1381.txt',\n",
       " 'cv257_10975.txt',\n",
       " 'cv095_28892.txt',\n",
       " 'cv787_13743.txt',\n",
       " 'cv356_25163.txt',\n",
       " 'cv448_14695.txt',\n",
       " 'cv994_12270.txt',\n",
       " 'cv455_29000.txt',\n",
       " 'cv956_11609.txt',\n",
       " 'cv136_11505.txt',\n",
       " 'cv851_20469.txt',\n",
       " 'cv933_23776.txt',\n",
       " 'cv729_10154.txt',\n",
       " 'cv942_17082.txt',\n",
       " 'cv663_13019.txt',\n",
       " 'cv855_20661.txt',\n",
       " 'cv832_23275.txt',\n",
       " 'cv603_17694.txt',\n",
       " 'cv862_14324.txt',\n",
       " 'cv761_12620.txt',\n",
       " 'cv849_15729.txt',\n",
       " 'cv618_8974.txt',\n",
       " 'cv020_8825.txt',\n",
       " 'cv355_16413.txt',\n",
       " 'cv099_10534.txt',\n",
       " 'cv401_12605.txt',\n",
       " 'cv672_28083.txt',\n",
       " 'cv330_29809.txt',\n",
       " 'cv681_9692.txt',\n",
       " 'cv220_29059.txt',\n",
       " 'cv004_11636.txt',\n",
       " 'cv476_16856.txt',\n",
       " 'cv733_9839.txt',\n",
       " 'cv398_15537.txt',\n",
       " 'cv270_6079.txt',\n",
       " 'cv223_29066.txt',\n",
       " 'cv184_2673.txt',\n",
       " 'cv162_10424.txt',\n",
       " 'cv176_12857.txt',\n",
       " 'cv717_15953.txt',\n",
       " 'cv464_15650.txt',\n",
       " 'cv631_4967.txt',\n",
       " 'cv297_10047.txt',\n",
       " 'cv525_16122.txt',\n",
       " 'cv757_10189.txt',\n",
       " 'cv277_19091.txt',\n",
       " 'cv316_6370.txt',\n",
       " 'cv334_10001.txt',\n",
       " 'cv512_15965.txt',\n",
       " 'cv218_24352.txt',\n",
       " 'cv337_29181.txt',\n",
       " 'cv953_6836.txt',\n",
       " 'cv043_15013.txt',\n",
       " 'cv064_24576.txt',\n",
       " 'cv129_16741.txt',\n",
       " 'cv818_10211.txt',\n",
       " 'cv206_14293.txt',\n",
       " 'cv591_23640.txt',\n",
       " 'cv860_13853.txt',\n",
       " 'cv698_15253.txt',\n",
       " 'cv236_11565.txt',\n",
       " 'cv377_7946.txt',\n",
       " 'cv050_11175.txt',\n",
       " 'cv047_1754.txt',\n",
       " 'cv513_6923.txt',\n",
       " 'cv822_20049.txt',\n",
       " 'cv710_22577.txt',\n",
       " 'cv105_17990.txt',\n",
       " 'cv087_1989.txt',\n",
       " 'cv294_11684.txt',\n",
       " 'cv317_24049.txt',\n",
       " 'cv798_23531.txt',\n",
       " 'cv975_10981.txt',\n",
       " 'cv743_15449.txt',\n",
       " 'cv032_22550.txt',\n",
       " 'cv144_5007.txt',\n",
       " 'cv981_14989.txt',\n",
       " 'cv629_14909.txt',\n",
       " 'cv556_14808.txt',\n",
       " 'cv485_26649.txt',\n",
       " 'cv289_6463.txt',\n",
       " 'cv691_5043.txt',\n",
       " 'cv641_12349.txt',\n",
       " 'cv086_18371.txt',\n",
       " 'cv352_5524.txt',\n",
       " 'cv387_11507.txt',\n",
       " 'cv805_19601.txt',\n",
       " 'cv713_29155.txt',\n",
       " 'cv800_12368.txt',\n",
       " 'cv171_13537.txt',\n",
       " 'cv544_5108.txt',\n",
       " 'cv708_28729.txt',\n",
       " 'cv814_18975.txt',\n",
       " 'cv821_29364.txt',\n",
       " 'cv331_8273.txt',\n",
       " 'cv411_15007.txt',\n",
       " 'cv139_12873.txt',\n",
       " 'cv638_2953.txt',\n",
       " 'cv662_13320.txt',\n",
       " 'cv820_22892.txt',\n",
       " 'cv440_15243.txt',\n",
       " 'cv728_16133.txt',\n",
       " 'cv809_5009.txt',\n",
       " 'cv443_21118.txt',\n",
       " 'cv853_29233.txt',\n",
       " 'cv260_13959.txt',\n",
       " 'cv986_13527.txt',\n",
       " 'cv341_24430.txt',\n",
       " 'cv202_10654.txt',\n",
       " 'cv416_11136.txt',\n",
       " 'cv984_12767.txt',\n",
       " 'cv539_20347.txt',\n",
       " 'cv741_11890.txt',\n",
       " 'cv018_20137.txt',\n",
       " 'cv179_9228.txt',\n",
       " 'cv770_10451.txt',\n",
       " 'cv944_13521.txt',\n",
       " 'cv985_6359.txt',\n",
       " 'cv921_12747.txt',\n",
       " 'cv205_9457.txt',\n",
       " 'cv468_15228.txt',\n",
       " 'cv038_9749.txt',\n",
       " 'cv917_29715.txt',\n",
       " 'cv778_17330.txt',\n",
       " 'cv968_24218.txt',\n",
       " 'cv811_21386.txt',\n",
       " 'cv189_22934.txt',\n",
       " 'cv488_19856.txt',\n",
       " 'cv934_19027.txt',\n",
       " 'cv559_0050.txt',\n",
       " 'cv677_17715.txt',\n",
       " 'cv028_26746.txt',\n",
       " 'cv516_11172.txt',\n",
       " 'cv041_21113.txt',\n",
       " 'cv600_23878.txt',\n",
       " 'cv303_27520.txt',\n",
       " 'cv369_12886.txt',\n",
       " 'cv637_1250.txt',\n",
       " 'cv459_20319.txt',\n",
       " 'cv683_12167.txt',\n",
       " 'cv001_18431.txt',\n",
       " 'cv926_17059.txt',\n",
       " 'cv654_18246.txt',\n",
       " 'cv720_5389.txt',\n",
       " 'cv295_15570.txt',\n",
       " 'cv676_21090.txt',\n",
       " 'cv852_27523.txt',\n",
       " 'cv777_10094.txt',\n",
       " 'cv193_5416.txt',\n",
       " 'cv666_18963.txt',\n",
       " 'cv931_17563.txt',\n",
       " 'cv196_29027.txt',\n",
       " 'cv278_13041.txt',\n",
       " 'cv780_7984.txt',\n",
       " 'cv572_18657.txt',\n",
       " 'cv841_3967.txt',\n",
       " 'cv570_29082.txt',\n",
       " 'cv548_17731.txt',\n",
       " 'cv752_24155.txt',\n",
       " 'cv056_13133.txt',\n",
       " 'cv378_20629.txt',\n",
       " 'cv432_14224.txt',\n",
       " 'cv211_9953.txt',\n",
       " 'cv436_19179.txt',\n",
       " 'cv122_7392.txt',\n",
       " 'cv449_8785.txt',\n",
       " 'cv397_29023.txt',\n",
       " 'cv586_7543.txt',\n",
       " 'cv625_12440.txt',\n",
       " 'cv413_7398.txt',\n",
       " 'cv386_10080.txt',\n",
       " 'cv063_28997.txt',\n",
       " 'cv147_21193.txt',\n",
       " 'cv690_5619.txt',\n",
       " 'cv989_15824.txt',\n",
       " 'cv151_15771.txt',\n",
       " 'cv754_7216.txt',\n",
       " 'cv555_23922.txt',\n",
       " 'cv657_24513.txt',\n",
       " 'cv837_27325.txt',\n",
       " 'cv266_25779.txt',\n",
       " 'cv457_18453.txt',\n",
       " 'cv114_18398.txt',\n",
       " 'cv307_25270.txt',\n",
       " 'cv290_11084.txt',\n",
       " 'cv970_18450.txt',\n",
       " 'cv912_5674.txt',\n",
       " 'cv143_19666.txt',\n",
       " 'cv957_8737.txt',\n",
       " 'cv906_11491.txt',\n",
       " 'cv195_14528.txt',\n",
       " 'cv178_12972.txt',\n",
       " 'cv635_10022.txt',\n",
       " 'cv142_22516.txt',\n",
       " 'cv082_11080.txt',\n",
       " 'cv810_12458.txt',\n",
       " 'cv424_8831.txt',\n",
       " 'cv265_10814.txt',\n",
       " 'cv255_13683.txt',\n",
       " 'cv724_13681.txt',\n",
       " 'cv128_29627.txt',\n",
       " 'cv671_5054.txt',\n",
       " 'cv784_14394.txt',\n",
       " 'cv491_12145.txt',\n",
       " 'cv243_20728.txt',\n",
       " 'cv452_5088.txt',\n",
       " 'cv620_24265.txt',\n",
       " 'cv954_18628.txt',\n",
       " 'cv423_11155.txt',\n",
       " 'cv065_15248.txt',\n",
       " 'cv098_15435.txt',\n",
       " 'cv952_25240.txt',\n",
       " 'cv385_29741.txt',\n",
       " 'cv388_12009.txt',\n",
       " 'cv165_22619.txt',\n",
       " 'cv124_4122.txt',\n",
       " 'cv014_13924.txt',\n",
       " 'cv524_23627.txt',\n",
       " 'cv899_16014.txt',\n",
       " 'cv215_22240.txt',\n",
       " 'cv422_9381.txt',\n",
       " 'cv225_29224.txt',\n",
       " 'cv089_11418.txt',\n",
       " 'cv619_12462.txt',\n",
       " 'cv546_11767.txt',\n",
       " 'cv593_10987.txt',\n",
       " 'cv217_28842.txt',\n",
       " 'cv530_16212.txt',\n",
       " 'cv992_11962.txt',\n",
       " 'cv839_21467.txt',\n",
       " 'cv301_12146.txt',\n",
       " 'cv774_13845.txt',\n",
       " 'cv112_11193.txt',\n",
       " 'cv786_22497.txt',\n",
       " 'cv608_23231.txt',\n",
       " 'cv324_7082.txt',\n",
       " 'cv033_24444.txt',\n",
       " 'cv175_6964.txt',\n",
       " 'cv699_7223.txt',\n",
       " 'cv279_18329.txt',\n",
       " 'cv866_29691.txt',\n",
       " 'cv529_10420.txt',\n",
       " 'cv617_9322.txt',\n",
       " 'cv977_4938.txt',\n",
       " 'cv313_18198.txt',\n",
       " 'cv016_4659.txt',\n",
       " 'cv687_21100.txt',\n",
       " 'cv284_19119.txt',\n",
       " 'cv993_29737.txt',\n",
       " 'cv915_8841.txt',\n",
       " 'cv075_6500.txt',\n",
       " 'cv141_15686.txt',\n",
       " 'cv365_11576.txt',\n",
       " 'cv868_11948.txt',\n",
       " 'cv576_14094.txt',\n",
       " 'cv072_6169.txt',\n",
       " 'cv813_6534.txt',\n",
       " 'cv345_9954.txt',\n",
       " 'cv309_22571.txt',\n",
       " 'cv936_15954.txt',\n",
       " 'cv492_18271.txt',\n",
       " 'cv069_10801.txt',\n",
       " 'cv756_22540.txt',\n",
       " 'cv693_18063.txt',\n",
       " 'cv939_10583.txt',\n",
       " 'cv782_19526.txt',\n",
       " 'cv834_22195.txt',\n",
       " 'cv582_6559.txt',\n",
       " 'cv268_18834.txt',\n",
       " 'cv281_23253.txt',\n",
       " 'cv904_24353.txt',\n",
       " 'cv895_21022.txt',\n",
       " 'cv480_19817.txt',\n",
       " 'cv840_16321.txt',\n",
       " 'cv258_5792.txt',\n",
       " 'cv833_11053.txt',\n",
       " 'cv588_13008.txt',\n",
       " 'cv875_5754.txt',\n",
       " 'cv458_8604.txt',\n",
       " 'cv505_12090.txt',\n",
       " 'cv830_6014.txt',\n",
       " 'cv579_11605.txt',\n",
       " 'cv318_10493.txt',\n",
       " 'cv526_12083.txt',\n",
       " 'cv534_14083.txt',\n",
       " 'cv434_5793.txt',\n",
       " 'cv567_29611.txt',\n",
       " 'cv501_11657.txt',\n",
       " 'cv997_5046.txt',\n",
       " 'cv101_10175.txt',\n",
       " 'cv230_7428.txt',\n",
       " 'cv031_18452.txt',\n",
       " 'cv901_11017.txt',\n",
       " 'cv010_29198.txt',\n",
       " 'cv438_8043.txt',\n",
       " 'cv021_15838.txt',\n",
       " 'cv111_11473.txt',\n",
       " 'cv150_12916.txt',\n",
       " 'cv315_11629.txt',\n",
       " 'cv012_29576.txt',\n",
       " 'cv402_14425.txt',\n",
       " 'cv656_24201.txt',\n",
       " 'cv514_11187.txt',\n",
       " 'cv310_13091.txt',\n",
       " 'cv537_12370.txt',\n",
       " 'cv074_6875.txt',\n",
       " 'cv226_2618.txt',\n",
       " 'cv042_10982.txt',\n",
       " 'cv344_5312.txt',\n",
       " 'cv073_21785.txt',\n",
       " 'cv844_12690.txt',\n",
       " 'cv626_7410.txt',\n",
       " 'cv888_24435.txt',\n",
       " 'cv896_16071.txt',\n",
       " 'cv615_14182.txt',\n",
       " 'cv571_29366.txt',\n",
       " 'cv705_11059.txt',\n",
       " 'cv946_18658.txt',\n",
       " 'cv224_17661.txt',\n",
       " 'cv113_23102.txt',\n",
       " 'cv843_15544.txt',\n",
       " 'cv919_16380.txt',\n",
       " 'cv616_29319.txt',\n",
       " 'cv134_22246.txt',\n",
       " 'cv990_11591.txt',\n",
       " 'cv484_25054.txt',\n",
       " 'cv935_23841.txt',\n",
       " 'cv110_27788.txt',\n",
       " 'cv971_10874.txt',\n",
       " 'cv806_8842.txt',\n",
       " 'cv979_18921.txt',\n",
       " 'cv883_27751.txt',\n",
       " 'cv088_24113.txt',\n",
       " 'cv319_14727.txt',\n",
       " 'cv854_17740.txt',\n",
       " 'cv732_12245.txt',\n",
       " 'cv739_11209.txt',\n",
       " 'cv235_10217.txt',\n",
       " 'cv794_15868.txt',\n",
       " 'cv054_4230.txt',\n",
       " 'cv058_8025.txt',\n",
       " 'cv435_23110.txt',\n",
       " 'cv889_21430.txt',\n",
       " 'cv446_11353.txt',\n",
       " 'cv156_10481.txt',\n",
       " 'cv242_10638.txt',\n",
       " 'cv108_15571.txt',\n",
       " 'cv039_6170.txt',\n",
       " 'cv976_10267.txt',\n",
       " 'cv838_24728.txt',\n",
       " 'cv611_21120.txt',\n",
       " 'cv595_25335.txt',\n",
       " 'cv560_17175.txt',\n",
       " 'cv504_29243.txt',\n",
       " 'cv383_13116.txt',\n",
       " 'cv340_13287.txt',\n",
       " 'cv483_16378.txt',\n",
       " 'cv059_28885.txt',\n",
       " 'cv037_18510.txt',\n",
       " 'cv522_5583.txt',\n",
       " 'cv924_29540.txt',\n",
       " 'cv267_14952.txt',\n",
       " 'cv563_17257.txt',\n",
       " 'cv370_5221.txt',\n",
       " 'cv005_29443.txt',\n",
       " 'cv763_14729.txt',\n",
       " 'cv183_18612.txt',\n",
       " 'cv973_10066.txt',\n",
       " 'cv203_17986.txt',\n",
       " 'cv002_15918.txt',\n",
       " 'cv829_20289.txt',\n",
       " 'cv140_7479.txt',\n",
       " 'cv096_11474.txt',\n",
       " 'cv262_12649.txt',\n",
       " 'cv366_10221.txt',\n",
       " 'cv867_16661.txt',\n",
       " 'cv726_4719.txt',\n",
       " 'cv874_11236.txt',\n",
       " 'cv945_12160.txt',\n",
       " 'cv030_21593.txt',\n",
       " 'cv709_10529.txt',\n",
       " 'cv360_8398.txt',\n",
       " 'cv454_2053.txt',\n",
       " 'cv601_23453.txt',\n",
       " 'cv299_16214.txt',\n",
       " 'cv137_15422.txt',\n",
       " 'cv721_29121.txt',\n",
       " 'cv558_29507.txt',\n",
       " 'cv044_16969.txt',\n",
       " 'cv094_27889.txt',\n",
       " 'cv622_8147.txt',\n",
       " 'cv466_18722.txt',\n",
       " 'cv911_20260.txt',\n",
       " 'cv052_29378.txt',\n",
       " 'cv648_15792.txt',\n",
       " 'cv748_12786.txt',\n",
       " 'cv846_29497.txt',\n",
       " 'cv767_14062.txt',\n",
       " 'cv323_29805.txt',\n",
       " 'cv320_9530.txt',\n",
       " 'cv744_10038.txt',\n",
       " 'cv371_7630.txt',\n",
       " 'cv399_2877.txt',\n",
       " 'cv885_12318.txt',\n",
       " 'cv462_19350.txt',\n",
       " 'cv584_29722.txt',\n",
       " 'cv684_11798.txt',\n",
       " 'cv597_26360.txt',\n",
       " 'cv655_11154.txt',\n",
       " 'cv496_10530.txt',\n",
       " 'cv714_18502.txt',\n",
       " 'cv755_23616.txt',\n",
       " 'cv159_29505.txt',\n",
       " 'cv066_10821.txt',\n",
       " 'cv269_21732.txt',\n",
       " 'cv775_16237.txt',\n",
       " 'cv167_16376.txt',\n",
       " 'cv961_5682.txt',\n",
       " 'cv486_9799.txt',\n",
       " 'cv123_11182.txt',\n",
       " 'cv322_20318.txt',\n",
       " 'cv439_15970.txt',\n",
       " 'cv545_12014.txt',\n",
       " 'cv368_10466.txt',\n",
       " 'cv186_2269.txt',\n",
       " 'cv372_6552.txt',\n",
       " 'cv080_13465.txt',\n",
       " 'cv679_28559.txt',\n",
       " 'cv079_11933.txt',\n",
       " 'cv229_13611.txt',\n",
       " 'cv406_21020.txt',\n",
       " 'cv051_10306.txt',\n",
       " 'cv177_10367.txt',\n",
       " 'cv149_15670.txt',\n",
       " 'cv100_11528.txt',\n",
       " 'cv312_29377.txt',\n",
       " 'cv828_19831.txt',\n",
       " 'cv879_14903.txt',\n",
       " 'cv950_12350.txt',\n",
       " 'cv969_13250.txt',\n",
       " 'cv169_23778.txt',\n",
       " 'cv589_12064.txt',\n",
       " 'cv017_22464.txt',\n",
       " 'cv553_26915.txt',\n",
       " 'cv055_8338.txt',\n",
       " 'cv538_28667.txt',\n",
       " 'cv760_8597.txt',\n",
       " 'cv644_17154.txt',\n",
       " 'cv157_29372.txt',\n",
       " 'cv292_7282.txt',\n",
       " 'cv373_20404.txt',\n",
       " 'cv508_16006.txt',\n",
       " 'cv342_19456.txt',\n",
       " 'cv650_14340.txt',\n",
       " 'cv727_4978.txt',\n",
       " 'cv667_18467.txt',\n",
       " 'cv300_22284.txt',\n",
       " 'cv328_10373.txt',\n",
       " 'cv643_29349.txt',\n",
       " 'cv191_29719.txt',\n",
       " 'cv881_13254.txt',\n",
       " 'cv801_25228.txt',\n",
       " 'cv856_29013.txt',\n",
       " 'cv920_29622.txt',\n",
       " 'cv900_10331.txt',\n",
       " 'cv234_20643.txt',\n",
       " 'cv248_13987.txt',\n",
       " 'cv154_9328.txt',\n",
       " 'cv612_5461.txt',\n",
       " 'cv523_16615.txt',\n",
       " 'cv996_11592.txt',\n",
       " 'cv431_7085.txt',\n",
       " 'cv130_17083.txt',\n",
       " 'cv393_29327.txt',\n",
       " 'cv396_17989.txt',\n",
       " 'cv447_27332.txt',\n",
       " 'cv640_5378.txt',\n",
       " 'cv132_5618.txt',\n",
       " 'cv395_10849.txt',\n",
       " 'cv628_19325.txt',\n",
       " 'cv471_16858.txt',\n",
       " 'cv796_15782.txt',\n",
       " 'cv528_10822.txt',\n",
       " 'cv886_18177.txt',\n",
       " 'cv194_12079.txt',\n",
       " 'cv419_13394.txt',\n",
       " 'cv298_23111.txt',\n",
       " 'cv173_4471.txt',\n",
       " 'cv219_18626.txt',\n",
       " 'cv898_14187.txt',\n",
       " 'cv061_8837.txt',\n",
       " 'cv008_29435.txt',\n",
       " 'cv026_29325.txt',\n",
       " 'cv520_12295.txt',\n",
       " 'cv502_10406.txt',\n",
       " 'cv824_8838.txt',\n",
       " 'cv409_29786.txt',\n",
       " 'cv680_10160.txt',\n",
       " 'cv927_10681.txt',\n",
       " 'cv542_18980.txt',\n",
       " 'cv308_5016.txt',\n",
       " 'cv185_28654.txt',\n",
       " 'cv873_18636.txt',\n",
       " 'cv197_29328.txt',\n",
       " 'cv962_9803.txt',\n",
       " 'cv475_21692.txt',\n",
       " 'cv632_9610.txt',\n",
       " 'cv557_11449.txt',\n",
       " 'cv380_7574.txt',\n",
       " 'cv510_23360.txt',\n",
       " 'cv314_14422.txt',\n",
       " 'cv735_18801.txt',\n",
       " 'cv605_11800.txt',\n",
       " 'cv024_6778.txt',\n",
       " 'cv497_26980.txt',\n",
       " 'cv465_22431.txt',\n",
       " 'cv692_15451.txt',\n",
       " 'cv264_12801.txt',\n",
       " 'cv606_15985.txt',\n",
       " 'cv621_14368.txt',\n",
       " 'cv138_12721.txt',\n",
       " 'cv472_29280.txt',\n",
       " 'cv762_13927.txt',\n",
       " 'cv390_11345.txt',\n",
       " 'cv799_18543.txt',\n",
       " 'cv090_0042.txt',\n",
       " 'cv937_9811.txt',\n",
       " 'cv850_16466.txt',\n",
       " 'cv908_16009.txt',\n",
       " 'cv490_17872.txt',\n",
       " 'cv930_13475.txt',\n",
       " 'cv583_29692.txt',\n",
       " 'cv428_11347.txt',\n",
       " 'cv521_15828.txt',\n",
       " 'cv420_28795.txt',\n",
       " 'cv107_24319.txt',\n",
       " 'cv495_14518.txt',\n",
       " 'cv311_16002.txt',\n",
       " 'cv725_10103.txt',\n",
       " 'cv902_12256.txt',\n",
       " 'cv959_14611.txt',\n",
       " 'cv246_28807.txt',\n",
       " 'cv928_9168.txt',\n",
       " 'cv271_13837.txt',\n",
       " 'cv807_21740.txt',\n",
       " 'cv704_15969.txt',\n",
       " 'cv253_10077.txt',\n",
       " 'cv412_24095.txt',\n",
       " 'cv566_8581.txt',\n",
       " 'cv718_11434.txt',\n",
       " 'cv222_17395.txt',\n",
       " 'cv750_10180.txt',\n",
       " 'cv493_12839.txt',\n",
       " 'cv965_26071.txt',\n",
       " 'cv237_19221.txt',\n",
       " 'cv797_6957.txt',\n",
       " 'cv554_13151.txt',\n",
       " 'cv109_21172.txt',\n",
       " 'cv461_19600.txt',\n",
       " 'cv405_20399.txt',\n",
       " 'cv995_21821.txt',\n",
       " 'cv623_15356.txt',\n",
       " 'cv163_10052.txt',\n",
       " 'cv517_19219.txt',\n",
       " 'cv287_15900.txt',\n",
       " 'cv469_20630.txt',\n",
       " 'cv776_20529.txt',\n",
       " 'cv499_10658.txt',\n",
       " 'cv444_9974.txt',\n",
       " 'cv170_3006.txt',\n",
       " 'cv543_5045.txt',\n",
       " 'cv453_10379.txt',\n",
       " 'cv251_22636.txt',\n",
       " 'cv948_24606.txt',\n",
       " 'cv547_16324.txt',\n",
       " 'cv665_29538.txt',\n",
       " 'cv613_21796.txt',\n",
       " 'cv897_10837.txt']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir('datasets/review_polarity/txt_sentoken/pos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "148d9824-de77-4f5d-abd9-0aeab1a024e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all the files from directory\n",
    "def process_train(directory):\n",
    "    documents = []\n",
    "    for filename in os.listdir(directory):\n",
    "        if not filename.startswith('cv9'):\n",
    "            path = directory + '/' + filename\n",
    "            docs = load_doc(path)\n",
    "            tokens = clean_doc(docs)\n",
    "            documents.append(tokens)\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0bc41dec-5cc4-41c0-adc2-740c39dcdaeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr = process_train('datasets/review_polarity/txt_sentoken/pos/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "75165ad8-9949-45b4-a6a4-0c2c044e3e80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "900"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "85c1574d-51a7-4cfd-8867-7fe839e7cfe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all the files from directory\n",
    "def process_test(directory):\n",
    "    documents = []\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.startswith('cv9'):\n",
    "            path = directory + '/' + filename\n",
    "            docs = load_doc(path)\n",
    "            tokens = clean_doc(docs)\n",
    "            documents.append(tokens)\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "62c5a0c0-2385-447c-807d-7d44e2734b13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "te = process_test('datasets/review_polarity/txt_sentoken/pos/')\n",
    "len(te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3c3abfbf-3c75-47df-813d-ceaac0b41bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_docs(directory, is_train):\n",
    "    documents = []\n",
    "    for filename in os.listdir(directory):\n",
    "        if is_train and filename.startswith('cv9'):\n",
    "            continue\n",
    "        if not is_train and not filename.startswith('cv9'):\n",
    "            continue\n",
    "        path = directory + '/' + filename\n",
    "        docs = load_doc(path)\n",
    "        tokens = clean_doc(docs)\n",
    "        documents.append(tokens)\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "74140337-e134-4a92-9a44-a20e8d3e4674",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "900"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr = process_docs('datasets/review_polarity/txt_sentoken/pos/', True)\n",
    "len(tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "87f1fc84-b83b-4c30-8702-6c824af2d0e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "te = process_docs('datasets/review_polarity/txt_sentoken/pos/', False)\n",
    "len(te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c2c7bcd1-006b-47dd-9867-2335edc09cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(is_train):\n",
    "    neg = process_docs('datasets/review_polarity/txt_sentoken/neg/', is_train)\n",
    "    pos = process_docs('datasets/review_polarity/txt_sentoken/pos/', is_train)\n",
    "    docs = neg + pos\n",
    "    labels = [0 for i in range((len(neg)))] + [1 for i in range((len(pos)))]\n",
    "    return docs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3f018263-60c2-48f6-8df1-c4486e4cd2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load training data\n",
    "train_data, train_labels = load_data(True)\n",
    "# load testing data\n",
    "test_data, test_labels = load_data(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a8e291b4-6483-46b7-9df7-1bfcd6958704",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1800, 1800)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data), len(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "14b8f6cf-1401-4756-8849-cadb0a019535",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 200)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_data), len(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4ed37042-32bd-43e6-92e4-53fd3d5d75bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795ad4a5-a102-41de-a6ae-a7da4d316741",
   "metadata": {},
   "source": [
    "#### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9594c584-488e-40eb-b798-72acf7c6f720",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tokenizer(lines):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8983cc25-cb3b-41a9-9062-b5b244aee13e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = create_tokenizer(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c719cfde-49de-4322-a3fa-b4ba3f634320",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode the training data\n",
    "x_train = tokenizer.texts_to_matrix(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e7e08092-c9a3-437f-8570-5e65cf2c4a97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1800, 36389)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c10cabd7-b804-4ea6-9008-65953069b72b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36388"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a588c584-961e-4132-9961-232deb846e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode the training data\n",
    "x_test = tokenizer.texts_to_matrix(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4a762c6b-c3f1-402a-b3fb-fb3c1bd0418e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 36389)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4850faea-ba24-4eb1-97db-3e8833f9d378",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_docs(tokenizer, max_length, docs):\n",
    "    encoded = tokenizer.texts_to_sequences(docs)\n",
    "    padded = pad_sequences(encoded, maxlen=max_length, padding='post')\n",
    "    return padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "984a7223-7b66-4fc3-a9b9-51b7c7c21f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = max([len(sent) for sent in train_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "294ee61c-3305-4208-a51a-9731d6a06411",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1335"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2f47dee2-70cd-4cdb-b469-53ec51d1ff88",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = encode_docs(tokenizer, max_length, train_data)\n",
    "x_test = encode_docs(tokenizer, max_length, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b15ce740-a722-4d22-ac75-0f78e0c964e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1800, 1335)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "32d3582b-7e26-4f26-86f0-8927737b55bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 1335)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc7c245-cc8f-456f-9348-617330916394",
   "metadata": {},
   "source": [
    "#### Build the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "50522581-bf5d-4c7f-8632-1d4e3acbbfd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Input, Conv1D, MaxPool1D, Flatten, Embedding\n",
    "from keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "1f9c46a3-1688-492a-936c-79a713b32bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "4fd24cec-edb8-4e60-b050-254b45bd3d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_model(max_length):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, 100, max_length))\n",
    "    model.add(Conv1D(filters=16, kernel_size=5, activation='relu'))\n",
    "    model.add(MaxPool1D(pool_size=2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(10, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', \n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "633301a5-f2a0-4f2c-89c9-171843900bb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class Embedding in module keras.src.layers.core.embedding:\n",
      "\n",
      "class Embedding(keras.src.layers.layer.Layer)\n",
      " |  Embedding(input_dim, output_dim, embeddings_initializer='uniform', embeddings_regularizer=None, embeddings_constraint=None, mask_zero=False, weights=None, lora_rank=None, **kwargs)\n",
      " |  \n",
      " |  Turns nonnegative integers (indexes) into dense vectors of fixed size.\n",
      " |  \n",
      " |  e.g. `[[4], [20]] -> [[0.25, 0.1], [0.6, -0.2]]`\n",
      " |  \n",
      " |  This layer can only be used on nonnegative integer inputs of a fixed range.\n",
      " |  \n",
      " |  Example:\n",
      " |  \n",
      " |  >>> model = keras.Sequential()\n",
      " |  >>> model.add(keras.layers.Embedding(1000, 64))\n",
      " |  >>> # The model will take as input an integer matrix of size (batch,\n",
      " |  >>> # input_length), and the largest integer (i.e. word index) in the input\n",
      " |  >>> # should be no larger than 999 (vocabulary size).\n",
      " |  >>> # Now model.output_shape is (None, 10, 64), where `None` is the batch\n",
      " |  >>> # dimension.\n",
      " |  >>> input_array = np.random.randint(1000, size=(32, 10))\n",
      " |  >>> model.compile('rmsprop', 'mse')\n",
      " |  >>> output_array = model.predict(input_array)\n",
      " |  >>> print(output_array.shape)\n",
      " |  (32, 10, 64)\n",
      " |  \n",
      " |  Args:\n",
      " |      input_dim: Integer. Size of the vocabulary,\n",
      " |          i.e. maximum integer index + 1.\n",
      " |      output_dim: Integer. Dimension of the dense embedding.\n",
      " |      embeddings_initializer: Initializer for the `embeddings`\n",
      " |          matrix (see `keras.initializers`).\n",
      " |      embeddings_regularizer: Regularizer function applied to\n",
      " |          the `embeddings` matrix (see `keras.regularizers`).\n",
      " |      embeddings_constraint: Constraint function applied to\n",
      " |          the `embeddings` matrix (see `keras.constraints`).\n",
      " |      mask_zero: Boolean, whether or not the input value 0 is a special\n",
      " |          \"padding\" value that should be masked out.\n",
      " |          This is useful when using recurrent layers which\n",
      " |          may take variable length input. If this is `True`,\n",
      " |          then all subsequent layers in the model need\n",
      " |          to support masking or an exception will be raised.\n",
      " |          If `mask_zero` is set to `True`, as a consequence,\n",
      " |          index 0 cannot be used in the vocabulary (`input_dim` should\n",
      " |          equal size of vocabulary + 1).\n",
      " |      weights: Optional floating-point matrix of size\n",
      " |          `(input_dim, output_dim)`. The initial embeddings values\n",
      " |          to use.\n",
      " |      lora_rank: Optional integer. If set, the layer's forward pass\n",
      " |          will implement LoRA (Low-Rank Adaptation)\n",
      " |          with the provided rank. LoRA sets the layer's embeddings\n",
      " |          matrix to non-trainable and replaces it with a delta over the\n",
      " |          original matrix, obtained via multiplying two lower-rank\n",
      " |          trainable matrices. This can be useful to reduce the\n",
      " |          computation cost of fine-tuning large embedding layers.\n",
      " |          You can also enable LoRA on an existing\n",
      " |          `Embedding` layer by calling `layer.enable_lora(rank)`.\n",
      " |  \n",
      " |  Input shape:\n",
      " |      2D tensor with shape: `(batch_size, input_length)`.\n",
      " |  \n",
      " |  Output shape:\n",
      " |      3D tensor with shape: `(batch_size, input_length, output_dim)`.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      Embedding\n",
      " |      keras.src.layers.layer.Layer\n",
      " |      keras.src.backend.tensorflow.layer.TFLayer\n",
      " |      keras.src.backend.tensorflow.trackable.KerasAutoTrackable\n",
      " |      tensorflow.python.trackable.autotrackable.AutoTrackable\n",
      " |      tensorflow.python.trackable.base.Trackable\n",
      " |      keras.src.ops.operation.Operation\n",
      " |      keras.src.saving.keras_saveable.KerasSaveable\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, input_dim, output_dim, embeddings_initializer='uniform', embeddings_regularizer=None, embeddings_constraint=None, mask_zero=False, weights=None, lora_rank=None, **kwargs)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  build(self, input_shape=None)\n",
      " |  \n",
      " |  call(self, inputs)\n",
      " |  \n",
      " |  compute_mask(self, inputs, mask=None)\n",
      " |  \n",
      " |  compute_output_shape(self, input_shape)\n",
      " |  \n",
      " |  enable_lora(self, rank, a_initializer='he_uniform', b_initializer='zeros')\n",
      " |  \n",
      " |  get_config(self)\n",
      " |      Returns the config of the object.\n",
      " |      \n",
      " |      An object config is a Python dictionary (serializable)\n",
      " |      containing the information needed to re-instantiate it.\n",
      " |  \n",
      " |  load_own_variables(self, store)\n",
      " |      Loads the state of the layer.\n",
      " |      \n",
      " |      You can override this method to take full control of how the state of\n",
      " |      the layer is loaded upon calling `keras.models.load_model()`.\n",
      " |      \n",
      " |      Args:\n",
      " |          store: Dict from which the state of the model will be loaded.\n",
      " |  \n",
      " |  quantize(self, mode, type_check=True)\n",
      " |  \n",
      " |  quantized_build(self, input_shape, mode)\n",
      " |  \n",
      " |  quantized_call(self, *args, **kwargs)\n",
      " |  \n",
      " |  save_own_variables(self, store)\n",
      " |      Saves the state of the layer.\n",
      " |      \n",
      " |      You can override this method to take full control of how the state of\n",
      " |      the layer is saved upon calling `model.save()`.\n",
      " |      \n",
      " |      Args:\n",
      " |          store: Dict where the state of the model will be saved.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties defined here:\n",
      " |  \n",
      " |  embeddings\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __annotations__ = {}\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from keras.src.layers.layer.Layer:\n",
      " |  \n",
      " |  __call__(self, *args, **kwargs)\n",
      " |      Call self as a function.\n",
      " |  \n",
      " |  __delattr__(self, name)\n",
      " |      Implement delattr(self, name).\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setattr__(self, name, value)\n",
      " |      Support self.foo = trackable syntax.\n",
      " |  \n",
      " |  __str__(self)\n",
      " |      Return str(self).\n",
      " |  \n",
      " |  add_loss(self, loss)\n",
      " |      Can be called inside of the `call()` method to add a scalar loss.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```python\n",
      " |      class MyLayer(Layer):\n",
      " |          ...\n",
      " |          def call(self, x):\n",
      " |              self.add_loss(ops.sum(x))\n",
      " |              return x\n",
      " |      ```\n",
      " |  \n",
      " |  add_metric(self, *args, **kwargs)\n",
      " |  \n",
      " |  add_variable(self, shape, initializer, dtype=None, trainable=True, autocast=True, regularizer=None, constraint=None, name=None)\n",
      " |      Add a weight variable to the layer.\n",
      " |      \n",
      " |      Alias of `add_weight()`.\n",
      " |  \n",
      " |  add_weight(self, shape=None, initializer=None, dtype=None, trainable=True, autocast=True, regularizer=None, constraint=None, aggregation='mean', name=None)\n",
      " |      Add a weight variable to the layer.\n",
      " |      \n",
      " |      Args:\n",
      " |          shape: Shape tuple for the variable. Must be fully-defined\n",
      " |              (no `None` entries). Defaults to `()` (scalar) if unspecified.\n",
      " |          initializer: Initializer object to use to populate the initial\n",
      " |              variable value, or string name of a built-in initializer\n",
      " |              (e.g. `\"random_normal\"`). If unspecified, defaults to\n",
      " |              `\"glorot_uniform\"` for floating-point variables and to `\"zeros\"`\n",
      " |              for all other types (e.g. int, bool).\n",
      " |          dtype: Dtype of the variable to create, e.g. `\"float32\"`. If\n",
      " |              unspecified, defaults to the layer's variable dtype\n",
      " |              (which itself defaults to `\"float32\"` if unspecified).\n",
      " |          trainable: Boolean, whether the variable should be trainable via\n",
      " |              backprop or whether its updates are managed manually. Defaults\n",
      " |              to `True`.\n",
      " |          autocast: Boolean, whether to autocast layers variables when\n",
      " |              accessing them. Defaults to `True`.\n",
      " |          regularizer: Regularizer object to call to apply penalty on the\n",
      " |              weight. These penalties are summed into the loss function\n",
      " |              during optimization. Defaults to `None`.\n",
      " |          constraint: Contrainst object to call on the variable after any\n",
      " |              optimizer update, or string name of a built-in constraint.\n",
      " |              Defaults to `None`.\n",
      " |          aggregation: String, one of `'mean'`, `'sum'`,\n",
      " |              `'only_first_replica'`. Annotates the variable with the type\n",
      " |              of multi-replica aggregation to be used for this variable\n",
      " |              when writing custom data parallel training loops.\n",
      " |          name: String name of the variable. Useful for debugging purposes.\n",
      " |  \n",
      " |  build_from_config(self, config)\n",
      " |      Builds the layer's states with the supplied config dict.\n",
      " |      \n",
      " |      By default, this method calls the `build(config[\"input_shape\"])` method,\n",
      " |      which creates weights based on the layer's input shape in the supplied\n",
      " |      config. If your config contains other information needed to load the\n",
      " |      layer's state, you should override this method.\n",
      " |      \n",
      " |      Args:\n",
      " |          config: Dict containing the input shape associated with this layer.\n",
      " |  \n",
      " |  compute_output_spec(self, *args, **kwargs)\n",
      " |  \n",
      " |  count_params(self)\n",
      " |      Count the total number of scalars composing the weights.\n",
      " |      \n",
      " |      Returns:\n",
      " |          An integer count.\n",
      " |  \n",
      " |  get_build_config(self)\n",
      " |      Returns a dictionary with the layer's input shape.\n",
      " |      \n",
      " |      This method returns a config dict that can be used by\n",
      " |      `build_from_config(config)` to create all states (e.g. Variables and\n",
      " |      Lookup tables) needed by the layer.\n",
      " |      \n",
      " |      By default, the config only contains the input shape that the layer\n",
      " |      was built with. If you're writing a custom layer that creates state in\n",
      " |      an unusual way, you should override this method to make sure this state\n",
      " |      is already created when Keras attempts to load its value upon model\n",
      " |      loading.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A dict containing the input shape associated with the layer.\n",
      " |  \n",
      " |  get_weights(self)\n",
      " |      Return the values of `layer.weights` as a list of NumPy arrays.\n",
      " |  \n",
      " |  set_weights(self, weights)\n",
      " |      Sets the values of `layer.weights` from a list of NumPy arrays.\n",
      " |  \n",
      " |  stateless_call(self, trainable_variables, non_trainable_variables, *args, return_losses=False, **kwargs)\n",
      " |      Call the layer without any side effects.\n",
      " |      \n",
      " |      Args:\n",
      " |          trainable_variables: List of trainable variables of the model.\n",
      " |          non_trainable_variables: List of non-trainable variables of the\n",
      " |              model.\n",
      " |          *args: Positional arguments to be passed to `call()`.\n",
      " |          return_losses: If `True`, `stateless_call()` will return the list of\n",
      " |              losses created during `call()` as part of its return values.\n",
      " |          **kwargs: Keyword arguments to be passed to `call()`.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A tuple. By default, returns `(outputs, non_trainable_variables)`.\n",
      " |              If `return_losses = True`, then returns\n",
      " |              `(outputs, non_trainable_variables, losses)`.\n",
      " |      \n",
      " |      Note: `non_trainable_variables` include not only non-trainable weights\n",
      " |      such as `BatchNormalization` statistics, but also RNG seed state\n",
      " |      (if there are any random operations part of the layer, such as dropout),\n",
      " |      and `Metric` state (if there are any metrics attached to the layer).\n",
      " |      These are all elements of state of the layer.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```python\n",
      " |      model = ...\n",
      " |      data = ...\n",
      " |      trainable_variables = model.trainable_variables\n",
      " |      non_trainable_variables = model.non_trainable_variables\n",
      " |      # Call the model with zero side effects\n",
      " |      outputs, non_trainable_variables = model.stateless_call(\n",
      " |          trainable_variables,\n",
      " |          non_trainable_variables,\n",
      " |          data,\n",
      " |      )\n",
      " |      # Attach the updated state to the model\n",
      " |      # (until you do this, the model is still in its pre-call state).\n",
      " |      for ref_var, value in zip(\n",
      " |          model.non_trainable_variables, non_trainable_variables\n",
      " |      ):\n",
      " |          ref_var.assign(value)\n",
      " |      ```\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods inherited from keras.src.layers.layer.Layer:\n",
      " |  \n",
      " |  __new__(cls, *args, **kwargs)\n",
      " |      Create and return a new object.  See help(type) for accurate signature.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from keras.src.layers.layer.Layer:\n",
      " |  \n",
      " |  compute_dtype\n",
      " |      The dtype of the computations performed by the layer.\n",
      " |  \n",
      " |  dtype\n",
      " |      Alias of `layer.variable_dtype`.\n",
      " |  \n",
      " |  input_dtype\n",
      " |      The dtype layer inputs should be converted to.\n",
      " |  \n",
      " |  losses\n",
      " |      List of scalar losses from `add_loss`, regularizers and sublayers.\n",
      " |  \n",
      " |  metrics\n",
      " |      List of all metrics.\n",
      " |  \n",
      " |  metrics_variables\n",
      " |      List of all metric variables.\n",
      " |  \n",
      " |  non_trainable_variables\n",
      " |      List of all non-trainable layer state.\n",
      " |      \n",
      " |      This extends `layer.non_trainable_weights` to include all state used by\n",
      " |      the layer including state for metrics and `SeedGenerator`s.\n",
      " |  \n",
      " |  non_trainable_weights\n",
      " |      List of all non-trainable weight variables of the layer.\n",
      " |      \n",
      " |      These are the weights that should not be updated by the optimizer during\n",
      " |      training. Unlike, `layer.non_trainable_variables` this excludes metric\n",
      " |      state and random seeds.\n",
      " |  \n",
      " |  path\n",
      " |      The path of the layer.\n",
      " |      \n",
      " |      If the layer has not been built yet, it will be `None`.\n",
      " |  \n",
      " |  quantization_mode\n",
      " |      The quantization mode of this layer, `None` if not quantized.\n",
      " |  \n",
      " |  trainable_variables\n",
      " |      List of all trainable layer state.\n",
      " |      \n",
      " |      This is equivalent to `layer.trainable_weights`.\n",
      " |  \n",
      " |  trainable_weights\n",
      " |      List of all trainable weight variables of the layer.\n",
      " |      \n",
      " |      These are the weights that get updated by the optimizer during training.\n",
      " |  \n",
      " |  variable_dtype\n",
      " |      The dtype of the state (weights) of the layer.\n",
      " |  \n",
      " |  variables\n",
      " |      List of all layer state, including random seeds.\n",
      " |      \n",
      " |      This extends `layer.weights` to include all state used by the layer\n",
      " |      including `SeedGenerator`s.\n",
      " |      \n",
      " |      Note that metrics variables are not included here, use\n",
      " |      `metrics_variables` to visit all the metric variables.\n",
      " |  \n",
      " |  weights\n",
      " |      List of all weight variables of the layer.\n",
      " |      \n",
      " |      Unlike, `layer.variables` this excludes metric state and random seeds.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from keras.src.layers.layer.Layer:\n",
      " |  \n",
      " |  dtype_policy\n",
      " |  \n",
      " |  input_spec\n",
      " |  \n",
      " |  supports_masking\n",
      " |      Whether this layer supports computing a mask using `compute_mask`.\n",
      " |  \n",
      " |  trainable\n",
      " |      Settable boolean, whether this layer should be trainable or not.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from tensorflow.python.trackable.base.Trackable:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from keras.src.ops.operation.Operation:\n",
      " |  \n",
      " |  symbolic_call(self, *args, **kwargs)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from keras.src.ops.operation.Operation:\n",
      " |  \n",
      " |  from_config(config) from builtins.type\n",
      " |      Creates an operation from its config.\n",
      " |      \n",
      " |      This method is the reverse of `get_config`, capable of instantiating the\n",
      " |      same operation from the config dictionary.\n",
      " |      \n",
      " |      Note: If you override this method, you might receive a serialized dtype\n",
      " |      config, which is a `dict`. You can deserialize it as follows:\n",
      " |      \n",
      " |      ```python\n",
      " |      if \"dtype\" in config and isinstance(config[\"dtype\"], dict):\n",
      " |          policy = dtype_policies.deserialize(config[\"dtype\"])\n",
      " |      ```\n",
      " |      \n",
      " |      Args:\n",
      " |          config: A Python dictionary, typically the output of `get_config`.\n",
      " |      \n",
      " |      Returns:\n",
      " |          An operation instance.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from keras.src.ops.operation.Operation:\n",
      " |  \n",
      " |  input\n",
      " |      Retrieves the input tensor(s) of a symbolic operation.\n",
      " |      \n",
      " |      Only returns the tensor(s) corresponding to the *first time*\n",
      " |      the operation was called.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Input tensor or list of input tensors.\n",
      " |  \n",
      " |  output\n",
      " |      Retrieves the output tensor(s) of a layer.\n",
      " |      \n",
      " |      Only returns the tensor(s) corresponding to the *first time*\n",
      " |      the operation was called.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Output tensor or list of output tensors.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from keras.src.saving.keras_saveable.KerasSaveable:\n",
      " |  \n",
      " |  __reduce__(self)\n",
      " |      __reduce__ is used to customize the behavior of `pickle.pickle()`.\n",
      " |      \n",
      " |      The method returns a tuple of two elements: a function, and a list of\n",
      " |      arguments to pass to that function.  In this case we just leverage the\n",
      " |      keras saving library.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(Embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "a05b4ab7-ebea-4c87-9133-62e40941f5c4",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Could not interpret initializer identifier: 1335",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[79], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mdefine_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[78], line 3\u001b[0m, in \u001b[0;36mdefine_model\u001b[0;34m(max_length)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefine_model\u001b[39m(max_length):\n\u001b[1;32m      2\u001b[0m     model \u001b[38;5;241m=\u001b[39m Sequential()\n\u001b[0;32m----> 3\u001b[0m     model\u001b[38;5;241m.\u001b[39madd(\u001b[43mEmbedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvocab_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m      4\u001b[0m     model\u001b[38;5;241m.\u001b[39madd(Conv1D(filters\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m, kernel_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m      5\u001b[0m     model\u001b[38;5;241m.\u001b[39madd(MaxPool1D(pool_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m))\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/layers/core/embedding.py:96\u001b[0m, in \u001b[0;36mEmbedding.__init__\u001b[0;34m(self, input_dim, output_dim, embeddings_initializer, embeddings_regularizer, embeddings_constraint, mask_zero, weights, lora_rank, **kwargs)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_dim \u001b[38;5;241m=\u001b[39m input_dim\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_dim \u001b[38;5;241m=\u001b[39m output_dim\n\u001b[0;32m---> 96\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings_initializer \u001b[38;5;241m=\u001b[39m \u001b[43minitializers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43membeddings_initializer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings_regularizer \u001b[38;5;241m=\u001b[39m regularizers\u001b[38;5;241m.\u001b[39mget(embeddings_regularizer)\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings_constraint \u001b[38;5;241m=\u001b[39m constraints\u001b[38;5;241m.\u001b[39mget(embeddings_constraint)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/initializers/__init__.py:161\u001b[0m, in \u001b[0;36mget\u001b[0;34m(identifier)\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 161\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    162\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not interpret initializer identifier: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00midentifier\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    163\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Could not interpret initializer identifier: 1335"
     ]
    }
   ],
   "source": [
    "model = define_model(max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "4b9964a8-cc3b-4981-8a49-d6da65a17626",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "This model has not yet been built. Build the model first by calling `build()` or by calling the model on a batch of data.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[76], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mplot_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_layer_activations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_layer_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m          \u001b[49m\u001b[43mshow_shapes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/utils/model_visualization.py:421\u001b[0m, in \u001b[0;36mplot_model\u001b[0;34m(model, to_file, show_shapes, show_dtype, show_layer_names, rankdir, expand_nested, dpi, show_layer_activations, show_trainable, **kwargs)\u001b[0m\n\u001b[1;32m    386\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Converts a Keras model to dot format and save to a file.\u001b[39;00m\n\u001b[1;32m    387\u001b[0m \n\u001b[1;32m    388\u001b[0m \u001b[38;5;124;03mExample:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;124;03m    This enables in-line display of the model plots in notebooks.\u001b[39;00m\n\u001b[1;32m    418\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    420\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m model\u001b[38;5;241m.\u001b[39mbuilt:\n\u001b[0;32m--> 421\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    422\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis model has not yet been built. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    423\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBuild the model first by calling `build()` or by calling \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    424\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe model on a batch of data.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    425\u001b[0m     )\n\u001b[1;32m    426\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m check_pydot():\n\u001b[1;32m    427\u001b[0m     message \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    428\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou must install pydot (`pip install pydot`) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    429\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor `plot_model` to work.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    430\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: This model has not yet been built. Build the model first by calling `build()` or by calling the model on a batch of data."
     ]
    }
   ],
   "source": [
    "plot_model(model, show_layer_activations=True, show_layer_names=True,\n",
    "          show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4510bd2-be56-4c24-8114-a834463691ac",
   "metadata": {},
   "source": [
    "#### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "1474e596-edda-4a02-9e1e-02f1ffc8d7f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-29 09:34:04.092133: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: INVALID_ARGUMENT: indices[3,88] = 36077 is not in [0, 25767)\n",
      "\t [[{{function_node __inference_one_step_on_data_1726}}{{node sequential_4_1/embedding_2_1/GatherV2}}]]\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Graph execution error:\n\nDetected at node sequential_4_1/embedding_2_1/GatherV2 defined at (most recent call last):\n  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n\n  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel_launcher.py\", line 17, in <module>\n\n  File \"/usr/local/lib/python3.10/dist-packages/traitlets/config/application.py\", line 1075, in launch_instance\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelapp.py\", line 701, in start\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/platform/asyncio.py\", line 205, in start\n\n  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n\n  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n\n  File \"/usr/lib/python3.10/asyncio/events.py\", line 80, in _run\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 534, in dispatch_queue\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 523, in process_one\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 429, in dispatch_shell\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 767, in execute_request\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py\", line 429, in do_execute\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3051, in run_cell\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3106, in _run_cell\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3311, in run_cell_async\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3493, in run_ast_nodes\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n\n  File \"/tmp/ipykernel_106273/1278436805.py\", line 1, in <module>\n\n  File \"/home/mitu/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/home/mitu/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py\", line 368, in fit\n\n  File \"/home/mitu/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py\", line 216, in function\n\n  File \"/home/mitu/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py\", line 129, in multi_step_on_iterator\n\n  File \"/home/mitu/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py\", line 110, in one_step_on_data\n\n  File \"/home/mitu/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py\", line 56, in train_step\n\n  File \"/home/mitu/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/home/mitu/.local/lib/python3.10/site-packages/keras/src/layers/layer.py\", line 899, in __call__\n\n  File \"/home/mitu/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/home/mitu/.local/lib/python3.10/site-packages/keras/src/ops/operation.py\", line 46, in __call__\n\n  File \"/home/mitu/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 156, in error_handler\n\n  File \"/home/mitu/.local/lib/python3.10/site-packages/keras/src/models/sequential.py\", line 213, in call\n\n  File \"/home/mitu/.local/lib/python3.10/site-packages/keras/src/models/functional.py\", line 182, in call\n\n  File \"/home/mitu/.local/lib/python3.10/site-packages/keras/src/ops/function.py\", line 171, in _run_through_graph\n\n  File \"/home/mitu/.local/lib/python3.10/site-packages/keras/src/models/functional.py\", line 632, in call\n\n  File \"/home/mitu/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/home/mitu/.local/lib/python3.10/site-packages/keras/src/layers/layer.py\", line 899, in __call__\n\n  File \"/home/mitu/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/home/mitu/.local/lib/python3.10/site-packages/keras/src/ops/operation.py\", line 46, in __call__\n\n  File \"/home/mitu/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 156, in error_handler\n\n  File \"/home/mitu/.local/lib/python3.10/site-packages/keras/src/layers/core/embedding.py\", line 140, in call\n\n  File \"/home/mitu/.local/lib/python3.10/site-packages/keras/src/ops/numpy.py\", line 5239, in take\n\n  File \"/home/mitu/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/numpy.py\", line 2063, in take\n\nindices[3,88] = 36077 is not in [0, 25767)\n\t [[{{node sequential_4_1/embedding_2_1/GatherV2}}]] [Op:__inference_multi_step_on_iterator_1785]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[77], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_labels\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nDetected at node sequential_4_1/embedding_2_1/GatherV2 defined at (most recent call last):\n  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n\n  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel_launcher.py\", line 17, in <module>\n\n  File \"/usr/local/lib/python3.10/dist-packages/traitlets/config/application.py\", line 1075, in launch_instance\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelapp.py\", line 701, in start\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/platform/asyncio.py\", line 205, in start\n\n  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n\n  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n\n  File \"/usr/lib/python3.10/asyncio/events.py\", line 80, in _run\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 534, in dispatch_queue\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 523, in process_one\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 429, in dispatch_shell\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 767, in execute_request\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py\", line 429, in do_execute\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3051, in run_cell\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3106, in _run_cell\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3311, in run_cell_async\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3493, in run_ast_nodes\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n\n  File \"/tmp/ipykernel_106273/1278436805.py\", line 1, in <module>\n\n  File \"/home/mitu/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/home/mitu/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py\", line 368, in fit\n\n  File \"/home/mitu/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py\", line 216, in function\n\n  File \"/home/mitu/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py\", line 129, in multi_step_on_iterator\n\n  File \"/home/mitu/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py\", line 110, in one_step_on_data\n\n  File \"/home/mitu/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py\", line 56, in train_step\n\n  File \"/home/mitu/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/home/mitu/.local/lib/python3.10/site-packages/keras/src/layers/layer.py\", line 899, in __call__\n\n  File \"/home/mitu/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/home/mitu/.local/lib/python3.10/site-packages/keras/src/ops/operation.py\", line 46, in __call__\n\n  File \"/home/mitu/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 156, in error_handler\n\n  File \"/home/mitu/.local/lib/python3.10/site-packages/keras/src/models/sequential.py\", line 213, in call\n\n  File \"/home/mitu/.local/lib/python3.10/site-packages/keras/src/models/functional.py\", line 182, in call\n\n  File \"/home/mitu/.local/lib/python3.10/site-packages/keras/src/ops/function.py\", line 171, in _run_through_graph\n\n  File \"/home/mitu/.local/lib/python3.10/site-packages/keras/src/models/functional.py\", line 632, in call\n\n  File \"/home/mitu/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/home/mitu/.local/lib/python3.10/site-packages/keras/src/layers/layer.py\", line 899, in __call__\n\n  File \"/home/mitu/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/home/mitu/.local/lib/python3.10/site-packages/keras/src/ops/operation.py\", line 46, in __call__\n\n  File \"/home/mitu/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 156, in error_handler\n\n  File \"/home/mitu/.local/lib/python3.10/site-packages/keras/src/layers/core/embedding.py\", line 140, in call\n\n  File \"/home/mitu/.local/lib/python3.10/site-packages/keras/src/ops/numpy.py\", line 5239, in take\n\n  File \"/home/mitu/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/numpy.py\", line 2063, in take\n\nindices[3,88] = 36077 is not in [0, 25767)\n\t [[{{node sequential_4_1/embedding_2_1/GatherV2}}]] [Op:__inference_multi_step_on_iterator_1785]"
     ]
    }
   ],
   "source": [
    "model.fit(x_train, np.array(train_labels), epochs=10, batch_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "0b45b086-40b8-4329-b4f1-d3be66b16797",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m200/200\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8786 - loss: 0.2270\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.2301093488931656, 0.9049999713897705]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_test, np.array(test_labels), batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b889f5e7-56e2-40a8-a129-f43153719121",
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = 'Best movie ever! It was great, I will definitely recommend it.'\n",
    "text2 = 'This is the bad movie. Please dont watch it.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "595ed552-6d92-419c-9f5d-ea2c34bc79d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(review):\n",
    "    tokens = clean_doc(review)\n",
    "    tokens = ' '.join(tokens)\n",
    "    encoded = tokenizer.texts_to_matrix([tokens])\n",
    "    yhat = model.predict(encoded)\n",
    "    prob = yhat[0,0]\n",
    "    if prob > 0.5:\n",
    "        return 'Positive', prob\n",
    "    return 'Negative', 1-prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "7a7302da-8be4-461b-9330-1ac2f48fe0ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('Positive', 0.7157935)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sentiment(text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd32ce24-a3da-4f4e-b931-eed30685beeb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
